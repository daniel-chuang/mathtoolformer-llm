{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "116a82f1",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e772023f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from model.setup import setup_model, save_model, load_model, load_tokenizer\n",
    "from utils.console import isYes, printc, inputc, print_section\n",
    "from model.training import preprocess_for_training, train_model\n",
    "from data.gsm8k import prepare_gsm8k_dataset\n",
    "from data.svamp import prepare_svamp_dataset\n",
    "from data.arithmetic import prepare_arithmetic_datasets\n",
    "from evaluation.math_evaluation import evaluate_math_performance\n",
    "from evaluation.eval_pipeline import eval_model\n",
    "from constants import MODEL_NAME, INITIAL_SAVE_PATH, TOOL_FINETUNED_SAVE_PATH, DATASET, CHECKPOINTS, TOOL_TRAIN_DATASET_PATH, PURE_TRAIN_DATASET_PATH, EVAL_DATASET_PATH\n",
    "from data.arithmetic import combine_and_tokenize\n",
    "import wandb\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1ea1fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "current_epochs_tool = 1\n",
    "current_epochs_pure = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3516b64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[032m==================================================\u001b[0m\n",
      "\u001b[032m\n",
      "Loading Model\n",
      "\u001b[0m\n",
      "\u001b[032m==================================================\u001b[0m\n",
      "Loading model from local path: ./checkpoints\\pretrained\\qwen-initial\n",
      "Loaded model from saved path\n",
      "\u001b[032m==================================================\u001b[0m\n",
      "\u001b[032m\n",
      "Adding Tool Tokens\n",
      "\u001b[0m\n",
      "\u001b[032m==================================================\u001b[0m\n",
      "Added 2 special tokens to the tokenizer\n",
      "Resized model embeddings to 151648 tokens\n",
      "Special tokens: ['<|endoftext|>', '<tool:calculator>', '</tool>']\n"
     ]
    }
   ],
   "source": [
    "print_section(\"Loading Model\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# Try to load from saved path first, if it fails, download from HF\n",
    "try:\n",
    "    model, metadata = load_model(os.path.join(CHECKPOINTS, \"pretrained\", INITIAL_SAVE_PATH))\n",
    "    tokenizer = load_tokenizer(os.path.join(CHECKPOINTS, \"pretrained\", INITIAL_SAVE_PATH))\n",
    "    print(\"Loaded model from saved path\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Initial model not found. Setting up from {MODEL_NAME}\")\n",
    "    tokenizer, model, metadata = setup_model(MODEL_NAME)\n",
    "    save_model(model, tokenizer, os.path.join(CHECKPOINTS, \"pretrained\", INITIAL_SAVE_PATH))\n",
    "\n",
    "print_section(\"Adding Tool Tokens\")\n",
    "tool_tokens = {\n",
    "    \"additional_special_tokens\": [\n",
    "        \"<tool:calculator>\",\n",
    "        \"</tool>\",\n",
    "    ]\n",
    "}\n",
    "num_added = tokenizer.add_special_tokens(tool_tokens)\n",
    "print(f\"Added {num_added} special tokens to the tokenizer\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"Resized model embeddings to {len(tokenizer)} tokens\")\n",
    "print(\"Special tokens:\", tokenizer.all_special_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a320640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[032m==================================================\u001b[0m\n",
      "\u001b[032m\n",
      "Loading Data\n",
      "\u001b[0m\n",
      "\u001b[032m==================================================\u001b[0m\n",
      "Processing dataset configuration: arithmetic_1dc\n",
      "Processing dataset configuration: arithmetic_2da\n",
      "{'question': 'Question: What is 31 plus 72?\\nAnswer:', 'final_answer': '<tool:calculator>31 + 72</tool>'}\n",
      "{'question': 'Question: What is 12 plus 63?\\nAnswer:', 'final_answer': '<tool:calculator>12 + 63</tool>'}\n",
      "{'question': 'Question: What is 46 plus 53?\\nAnswer:', 'final_answer': '<tool:calculator>46 + 53</tool>'}\n",
      "{'question': 'Question: What is 46 plus 53?\\nAnswer:', 'final_answer': '<tool:calculator>46 + 53</tool>'}\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "print_section(\"Loading Data\")\n",
    "# Prepare datasets\n",
    "dataset = prepare_arithmetic_datasets()\n",
    "train_data = dataset[\"train_dict\"]\n",
    "test_data = dataset[\"test_dict\"]\n",
    "train_transformed_data = dataset[\"train_transformed_dict\"]\n",
    "test_transformed_data = dataset[\"test_transformed_dict\"]\n",
    "print(train_transformed_data['arithmetic_2da'][3])\n",
    "print(test_transformed_data['arithmetic_2da'][3])\n",
    "print(train_transformed_data['arithmetic_2da'][-1])\n",
    "print(train_transformed_data['arithmetic_2da'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525ff7c9",
   "metadata": {},
   "source": [
    "# Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "315c4980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[032m==================================================\u001b[0m\n",
      "\u001b[032m\n",
      "Pretrained Model Evaluation\n",
      "\u001b[0m\n",
      "\u001b[032m==================================================\u001b[0m\n",
      "Evaluating math performance on dataset arithmetic_1dc\n",
      "Evaluating math performance...\n",
      "------\n",
      "Question: What is (4 - 2) + 7?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 9\n",
      "Model: 9\n",
      "Correct: 1, Total: 1\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (1 + 8) - 8?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# PRETAINED MODEL EVALUATION\u001b[39;00m\n\u001b[32m      2\u001b[39m print_section(\u001b[33m\"\u001b[39m\u001b[33mPretrained Model Evaluation\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATASET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_tool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Programming\\Classes\\CS4782\\final_project\\code\\evaluation\\eval_pipeline.py:21\u001b[39m, in \u001b[36meval_model\u001b[39m\u001b[34m(model_name, dataset_name, test_datasets, model, tokenizer, use_tool)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, test_dataset \u001b[38;5;129;01min\u001b[39;00m test_datasets.items():\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvaluating math performance on dataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     results = \u001b[43mevaluate_math_performance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_tool\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkey\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMath Evaluation Results for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m, results[\u001b[33m'\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# Aggregate results\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Programming\\Classes\\CS4782\\final_project\\code\\evaluation\\math_evaluation.py:60\u001b[39m, in \u001b[36mevaluate_math_performance\u001b[39m\u001b[34m(model, tokenizer, test_dataset, use_tool, dataset_name, model_name)\u001b[39m\n\u001b[32m     57\u001b[39m     prompt = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAnswer:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m------\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m response = \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_tool\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Extract the last number mentioned in the response\u001b[39;00m\n\u001b[32m     63\u001b[39m numbers = re.findall(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m-?\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md+\u001b[39m\u001b[33m'\u001b[39m, response)  \u001b[38;5;66;03m# Find all numbers in the response (with optional minus sign)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Programming\\Classes\\CS4782\\final_project\\code\\inference\\tool_execution.py:98\u001b[39m, in \u001b[36minference\u001b[39m\u001b[34m(model, tokenizer, prompt, max_new_tokens, use_tool)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# stop_sequences = [\"Question:\", \"\\n\\n\", \"Answer:\"]  # Patterns that indicate the end of an answer\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# stopping_criteria = StoppingCriteriaList([\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m#     StopOnTokens(stop_sequences, tokenizer, model.device)\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# ])\u001b[39;00m\n\u001b[32m     94\u001b[39m \n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Initial generation\u001b[39;00m\n\u001b[32m     96\u001b[39m inputs = tokenizer(prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(model.device)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m outputs = \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Get the generated text\u001b[39;00m\n\u001b[32m    101\u001b[39m generated_text = tokenizer.decode(outputs[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Programming\\Classes\\CS4782\\final_project\\code\\inference\\generate.py:5\u001b[39m, in \u001b[36mgenerate\u001b[39m\u001b[34m(inputs, model, tokenizer, max_new_tokens)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(inputs, model, tokenizer, max_new_tokens=\u001b[32m150\u001b[39m):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m         outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# do_sample=True,\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m            \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\transformers\\generation\\utils.py:2465\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2457\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2458\u001b[39m         input_ids=input_ids,\n\u001b[32m   2459\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2460\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2461\u001b[39m         **model_kwargs,\n\u001b[32m   2462\u001b[39m     )\n\u001b[32m   2464\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2465\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2470\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2472\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2476\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2477\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2478\u001b[39m         input_ids=input_ids,\n\u001b[32m   2479\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2480\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2481\u001b[39m         **model_kwargs,\n\u001b[32m   2482\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\transformers\\generation\\utils.py:3434\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3432\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3434\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3436\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3437\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3438\u001b[39m     outputs,\n\u001b[32m   3439\u001b[39m     model_kwargs,\n\u001b[32m   3440\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3441\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\transformers\\utils\\generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:823\u001b[39m, in \u001b[36mQwen2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    818\u001b[39m output_hidden_states = (\n\u001b[32m    819\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    820\u001b[39m )\n\u001b[32m    822\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    836\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    837\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\transformers\\utils\\generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:549\u001b[39m, in \u001b[36mQwen2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    537\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    538\u001b[39m         partial(decoder_layer.\u001b[34m__call__\u001b[39m, **flash_attn_kwargs),\n\u001b[32m    539\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    546\u001b[39m         position_embeddings,\n\u001b[32m    547\u001b[39m     )\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:262\u001b[39m, in \u001b[36mQwen2DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    261\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    275\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:207\u001b[39m, in \u001b[36mQwen2Attention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    194\u001b[39m attn_output, attn_weights = attention_interface(\n\u001b[32m    195\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    196\u001b[39m     query_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    203\u001b[39m     **kwargs,\n\u001b[32m    204\u001b[39m )\n\u001b[32m    206\u001b[39m attn_output = attn_output.reshape(*input_shape, -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m attn_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mo_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, attn_weights\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# PRETAINED MODEL EVALUATION\n",
    "print_section(\"Pretrained Model Evaluation\")\n",
    "eval_model(MODEL_NAME, DATASET, test_data, model, tokenizer, use_tool=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081a6677",
   "metadata": {},
   "source": [
    "# Toolformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13379eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOOLFORMER FINE TUNING TRAINING\n",
    "print_section(\"Toolformer Fine Tuning Training\")\n",
    "\n",
    "# Prepare the training data based on dataset type\n",
    "train_dataset = combine_and_tokenize(train_transformed_data, tokenizer, path=TOOL_TRAIN_DATASET_PATH)\n",
    "\n",
    "# Create a small evaluation dataset directly instead of using combine_and_tokenize\n",
    "eval_examples = []\n",
    "for config_name, config_dataset in test_transformed_data.items():\n",
    "    # Take at most 5 examples from each configuration\n",
    "    sample_size = 1\n",
    "    for i in range(sample_size):\n",
    "        if isinstance(config_dataset[i], dict):\n",
    "            eval_examples.append({\n",
    "                \"question\": config_dataset[i][\"question\"],\n",
    "                \"final_answer\": config_dataset[i][\"final_answer\"]\n",
    "            })\n",
    "\n",
    "# Create the evaluation dataset directly\n",
    "eval_dataset = Dataset.from_list(eval_examples)\n",
    "eval_dataset = eval_dataset.map(\n",
    "    lambda examples: preprocess_for_training(examples, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\"Created evaluation dataset with {len(eval_dataset)} examples for monitoring\")\n",
    "\n",
    "# Load previous model if it exists\n",
    "try:\n",
    "    previous_path = os.path.join(CHECKPOINTS, \"finetuned\", TOOL_FINETUNED_SAVE_PATH)\n",
    "    tokenizer, model, metadata = load_model(previous_path)\n",
    "    \n",
    "    # Get total epochs from metadata\n",
    "    total_epochs = metadata.get(\"total_epochs\", 0) + current_epochs_tool\n",
    "    print(f\"Continuing training from {metadata.get('total_epochs', 0)} epochs to {total_epochs} epochs\")\n",
    "except FileNotFoundError:\n",
    "    # Start fresh training\n",
    "    total_epochs = current_epochs_tool\n",
    "    print(f\"Starting fresh training for {current_epochs_tool} epochs\")\n",
    "\n",
    "# Train the model\n",
    "model, tokenizer, metadata = train_model(model, tokenizer, train_dataset, num_epochs=current_epochs_tool, eval_dataset=eval_dataset)\n",
    "\n",
    "# Save with updated epoch count\n",
    "saved_path = save_model(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    os.path.join(CHECKPOINTS, \"finetuned\", TOOL_FINETUNED_SAVE_PATH),\n",
    "    epochs=current_epochs_tool,\n",
    "    total_epochs=total_epochs\n",
    ")\n",
    "print(f\"Saved fine-tuned model to {saved_path} (Total epochs: {total_epochs})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d134b92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[032m==================================================\u001b[0m\n",
      "\u001b[032m\n",
      "Latest Checkpoint Evaluation\n",
      "\u001b[0m\n",
      "\u001b[032m==================================================\u001b[0m\n",
      "Loading model from local path: .\\toolformer_model\\checkpoint-225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[032m==================================================\u001b[0m\n",
      "\u001b[032m\n",
      "Most recent training Model Evaluation\n",
      "\u001b[0m\n",
      "\u001b[032m==================================================\u001b[0m\n",
      "Evaluating math performance on dataset arithmetic_1dc\n",
      "Evaluating math performance...\n",
      "------\n",
      "Question: What is (4 - 2) + 7?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 - 2) + 7?\n",
      "Answer:\n",
      "\n",
      "Answer: 11\n",
      "--------------------\n",
      "Expected: 9\n",
      "Model: 11\n",
      "Correct: 0, Total: 1\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (1 + 8) - 8?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (1 + 8) - 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 1\n",
      "Model: 1\n",
      "Correct: 1, Total: 2\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 - 3) - 4?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 - 3) - 4?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: -3\n",
      "Model: 1\n",
      "Correct: 1, Total: 3\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (1 + 2) - 7?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (1 + 2) - 7?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: -4\n",
      "Model: 1\n",
      "Correct: 1, Total: 4\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (1 * 3) - 6?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (1 * 3) - 6?\n",
      "Answer:\n",
      "\n",
      "Answer: 3\n",
      "--------------------\n",
      "Expected: -3\n",
      "Model: 3\n",
      "Correct: 1, Total: 5\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (3 + 7) * 2?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (3 + 7) * 2?\n",
      "Answer:\n",
      "\n",
      "Answer: 17\n",
      "--------------------\n",
      "Expected: 20\n",
      "Model: 17\n",
      "Correct: 1, Total: 6\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 * 5) - 9?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 * 5) - 9?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 31\n",
      "Model: 1\n",
      "Correct: 1, Total: 7\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 + 7) * 6?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 + 7) * 6?\n",
      "Answer:\n",
      "\n",
      "Answer: 54\n",
      "--------------------\n",
      "Expected: 66\n",
      "Model: 54\n",
      "Correct: 1, Total: 8\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 - 7) * 1?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 - 7) * 1?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 2\n",
      "Model: 1\n",
      "Correct: 1, Total: 9\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 * 8) * 8?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 * 8) * 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 12\n",
      "--------------------\n",
      "Expected: 256\n",
      "Model: 12\n",
      "Correct: 1, Total: 10\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 - 5) * 1?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 - 5) * 1?\n",
      "Answer:\n",
      "\n",
      "Answer: 2\n",
      "--------------------\n",
      "Expected: 2\n",
      "Model: 2\n",
      "Correct: 2, Total: 11\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 * 9) - 6?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 * 9) - 6?\n",
      "Answer:\n",
      "\n",
      "Answer: 13\n",
      "--------------------\n",
      "Expected: 66\n",
      "Model: 13\n",
      "Correct: 2, Total: 12\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (6 * 3) - 3?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (6 * 3) - 3?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 15\n",
      "Model: 1\n",
      "Correct: 2, Total: 13\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 * 5) - 8?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 * 5) - 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 37\n",
      "Model: 1\n",
      "Correct: 2, Total: 14\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 + 1) * 4?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 + 1) * 4?\n",
      "Answer:\n",
      "\n",
      "Answer: 21\n",
      "--------------------\n",
      "Expected: 32\n",
      "Model: 21\n",
      "Correct: 2, Total: 15\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 * 1) * 1?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 * 1) * 1?\n",
      "Answer:\n",
      "\n",
      "Answer: 7\n",
      "--------------------\n",
      "Expected: 7\n",
      "Model: 7\n",
      "Correct: 3, Total: 16\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (3 + 9) * 3?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (3 + 9) * 3?\n",
      "Answer:\n",
      "\n",
      "Answer: 30\n",
      "--------------------\n",
      "Expected: 36\n",
      "Model: 30\n",
      "Correct: 3, Total: 17\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (3 * 5) - 6?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (3 * 5) - 6?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 9\n",
      "Model: 1\n",
      "Correct: 3, Total: 18\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (5 - 3) - 9?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (5 - 3) - 9?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: -7\n",
      "Model: 1\n",
      "Correct: 3, Total: 19\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 * 9) + 5?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 * 9) + 5?\n",
      "Answer:\n",
      "\n",
      "Answer: 34\n",
      "--------------------\n",
      "Expected: 77\n",
      "Model: 34\n",
      "Correct: 3, Total: 20\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 + 6) * 2?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 + 6) * 2?\n",
      "Answer:\n",
      "\n",
      "Answer: 22\n",
      "--------------------\n",
      "Expected: 20\n",
      "Model: 22\n",
      "Correct: 3, Total: 21\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 * 3) * 3?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 * 3) * 3?\n",
      "Answer:\n",
      "\n",
      "Answer: 42\n",
      "--------------------\n",
      "Expected: 72\n",
      "Model: 42\n",
      "Correct: 3, Total: 22\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 + 8) + 7?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 + 8) + 7?\n",
      "Answer:\n",
      "\n",
      "Answer: 24\n",
      "--------------------\n",
      "Expected: 24\n",
      "Model: 24\n",
      "Correct: 4, Total: 23\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (6 - 7) * 3?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (6 - 7) * 3?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: -3\n",
      "Model: 1\n",
      "Correct: 4, Total: 24\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 - 2) - 8?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 - 2) - 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: -3\n",
      "Model: 1\n",
      "Correct: 4, Total: 25\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (3 + 7) + 9?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (3 + 7) + 9?\n",
      "Answer:\n",
      "\n",
      "Answer: 21\n",
      "--------------------\n",
      "Expected: 19\n",
      "Model: 21\n",
      "Correct: 4, Total: 26\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (5 * 1) - 8?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (5 * 1) - 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: -3\n",
      "Model: 1\n",
      "Correct: 4, Total: 27\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (5 + 2) - 2?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (5 + 2) - 2?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 5\n",
      "Model: 1\n",
      "Correct: 4, Total: 28\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (5 * 5) + 4?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (5 * 5) + 4?\n",
      "Answer:\n",
      "\n",
      "Answer: 19\n",
      "--------------------\n",
      "Expected: 29\n",
      "Model: 19\n",
      "Correct: 4, Total: 29\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 - 1) * 3?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 - 1) * 3?\n",
      "Answer:\n",
      "\n",
      "Answer: 11\n",
      "--------------------\n",
      "Expected: 21\n",
      "Model: 11\n",
      "Correct: 4, Total: 30\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 * 4) + 8?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 * 4) + 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 32\n",
      "--------------------\n",
      "Expected: 40\n",
      "Model: 32\n",
      "Correct: 4, Total: 31\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (5 - 5) - 2?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (5 - 5) - 2?\n",
      "Answer:\n",
      "\n",
      "Answer: 0\n",
      "--------------------\n",
      "Expected: -2\n",
      "Model: 0\n",
      "Correct: 4, Total: 32\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 * 5) - 4?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 * 5) - 4?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 31\n",
      "Model: 1\n",
      "Correct: 4, Total: 33\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (6 + 1) - 7?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (6 + 1) - 7?\n",
      "Answer:\n",
      "\n",
      "Answer: 0\n",
      "--------------------\n",
      "Expected: 0\n",
      "Model: 0\n",
      "Correct: 5, Total: 34\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (1 * 5) + 9?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (1 * 5) + 9?\n",
      "Answer:\n",
      "\n",
      "Answer: 14\n",
      "--------------------\n",
      "Expected: 14\n",
      "Model: 14\n",
      "Correct: 6, Total: 35\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 * 7) * 9?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 * 7) * 9?\n",
      "Answer:\n",
      "\n",
      "Answer: 102\n",
      "--------------------\n",
      "Expected: 252\n",
      "Model: 102\n",
      "Correct: 6, Total: 36\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (5 + 2) * 5?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (5 + 2) * 5?\n",
      "Answer:\n",
      "\n",
      "Answer: 35\n",
      "--------------------\n",
      "Expected: 35\n",
      "Model: 35\n",
      "Correct: 7, Total: 37\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (5 * 7) * 4?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (5 * 7) * 4?\n",
      "Answer:\n",
      "\n",
      "Answer: 40\n",
      "--------------------\n",
      "Expected: 140\n",
      "Model: 40\n",
      "Correct: 7, Total: 38\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (6 - 5) + 7?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (6 - 5) + 7?\n",
      "Answer:\n",
      "\n",
      "Answer: 14\n",
      "--------------------\n",
      "Expected: 8\n",
      "Model: 14\n",
      "Correct: 7, Total: 39\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (3 - 6) * 3?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (3 - 6) * 3?\n",
      "Answer:\n",
      "\n",
      "Answer: 3\n",
      "--------------------\n",
      "Expected: -9\n",
      "Model: 3\n",
      "Correct: 7, Total: 40\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 - 5) * 4?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 - 5) * 4?\n",
      "Answer:\n",
      "\n",
      "Answer: 12\n",
      "--------------------\n",
      "Expected: -4\n",
      "Model: 12\n",
      "Correct: 7, Total: 41\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 + 4) - 3?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 + 4) - 3?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 10\n",
      "Model: 1\n",
      "Correct: 7, Total: 42\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 + 4) + 7?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 + 4) + 7?\n",
      "Answer:\n",
      "\n",
      "Answer: 20\n",
      "--------------------\n",
      "Expected: 18\n",
      "Model: 20\n",
      "Correct: 7, Total: 43\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (5 * 7) * 1?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (5 * 7) * 1?\n",
      "Answer:\n",
      "\n",
      "Answer: 35\n",
      "--------------------\n",
      "Expected: 35\n",
      "Model: 35\n",
      "Correct: 8, Total: 44\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 - 5) + 5?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 - 5) + 5?\n",
      "Answer:\n",
      "\n",
      "Answer: 10\n",
      "--------------------\n",
      "Expected: 7\n",
      "Model: 10\n",
      "Correct: 8, Total: 45\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (1 * 4) * 8?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (1 * 4) * 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 32\n",
      "--------------------\n",
      "Expected: 32\n",
      "Model: 32\n",
      "Correct: 9, Total: 46\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 * 1) - 3?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 * 1) - 3?\n",
      "Answer:\n",
      "\n",
      "Answer: 0\n",
      "--------------------\n",
      "Expected: 4\n",
      "Model: 0\n",
      "Correct: 9, Total: 47\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 + 7) - 8?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 + 7) - 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 8\n",
      "Model: 1\n",
      "Correct: 9, Total: 48\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (2 + 7) + 7?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (2 + 7) + 7?\n",
      "Answer:\n",
      "\n",
      "Answer: 20\n",
      "--------------------\n",
      "Expected: 16\n",
      "Model: 20\n",
      "Correct: 9, Total: 49\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 * 6) - 5?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 * 6) - 5?\n",
      "Answer:\n",
      "\n",
      "Answer: 19\n",
      "--------------------\n",
      "Expected: 49\n",
      "Model: 19\n",
      "Correct: 9, Total: 50\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (5 + 8) - 1?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (5 + 8) - 1?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 12\n",
      "Model: 1\n",
      "Correct: 9, Total: 51\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 - 6) + 6?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 - 6) + 6?\n",
      "Answer:\n",
      "\n",
      "Answer: 13\n",
      "--------------------\n",
      "Expected: 7\n",
      "Model: 13\n",
      "Correct: 9, Total: 52\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (1 * 1) * 4?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (1 * 1) * 4?\n",
      "Answer:\n",
      "\n",
      "Answer: 4\n",
      "--------------------\n",
      "Expected: 4\n",
      "Model: 4\n",
      "Correct: 10, Total: 53\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 * 1) * 3?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 * 1) * 3?\n",
      "Answer:\n",
      "\n",
      "Answer: 21\n",
      "--------------------\n",
      "Expected: 21\n",
      "Model: 21\n",
      "Correct: 11, Total: 54\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (5 * 3) + 8?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (5 * 3) + 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 23\n",
      "--------------------\n",
      "Expected: 23\n",
      "Model: 23\n",
      "Correct: 12, Total: 55\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 - 5) * 3?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 - 5) * 3?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: -3\n",
      "Model: 1\n",
      "Correct: 12, Total: 56\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 - 9) - 4?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 - 9) - 4?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: -5\n",
      "Model: 1\n",
      "Correct: 12, Total: 57\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (1 - 7) * 8?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (1 - 7) * 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: -48\n",
      "Model: 1\n",
      "Correct: 12, Total: 58\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (2 - 4) + 6?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (2 - 4) + 6?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 4\n",
      "Model: 1\n",
      "Correct: 12, Total: 59\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 - 9) * 2?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 - 9) * 2?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: -10\n",
      "Model: 1\n",
      "Correct: 12, Total: 60\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 - 1) - 2?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 - 1) - 2?\n",
      "Answer:\n",
      "\n",
      "Answer: 0\n",
      "--------------------\n",
      "Expected: 1\n",
      "Model: 0\n",
      "Correct: 12, Total: 61\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 * 8) - 9?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 * 8) - 9?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 23\n",
      "Model: 1\n",
      "Correct: 12, Total: 62\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 - 9) - 1?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 - 9) - 1?\n",
      "Answer:\n",
      "\n",
      "Answer: 0\n",
      "--------------------\n",
      "Expected: -3\n",
      "Model: 0\n",
      "Correct: 12, Total: 63\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (3 + 1) - 5?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (3 + 1) - 5?\n",
      "Answer:\n",
      "\n",
      "Answer: 0\n",
      "--------------------\n",
      "Expected: -1\n",
      "Model: 0\n",
      "Correct: 12, Total: 64\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 + 1) * 6?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 + 1) * 6?\n",
      "Answer:\n",
      "\n",
      "Answer: 30\n",
      "--------------------\n",
      "Expected: 30\n",
      "Model: 30\n",
      "Correct: 13, Total: 65\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 * 4) + 8?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 * 4) + 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 31\n",
      "--------------------\n",
      "Expected: 36\n",
      "Model: 31\n",
      "Correct: 13, Total: 66\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (5 - 3) * 5?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (5 - 3) * 5?\n",
      "Answer:\n",
      "\n",
      "Answer: 10\n",
      "--------------------\n",
      "Expected: 10\n",
      "Model: 10\n",
      "Correct: 14, Total: 67\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (5 - 9) * 4?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (5 - 9) * 4?\n",
      "Answer:\n",
      "\n",
      "Answer: 14\n",
      "--------------------\n",
      "Expected: -16\n",
      "Model: 14\n",
      "Correct: 14, Total: 68\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 * 4) - 2?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 * 4) - 2?\n",
      "Answer:\n",
      "\n",
      "Answer: 11\n",
      "--------------------\n",
      "Expected: 34\n",
      "Model: 11\n",
      "Correct: 14, Total: 69\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 + 8) * 8?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 + 8) * 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 14\n",
      "--------------------\n",
      "Expected: 128\n",
      "Model: 14\n",
      "Correct: 14, Total: 70\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (6 * 9) + 2?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (6 * 9) + 2?\n",
      "Answer:\n",
      "\n",
      "Answer: 21\n",
      "--------------------\n",
      "Expected: 56\n",
      "Model: 21\n",
      "Correct: 14, Total: 71\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 - 8) - 9?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 - 8) - 9?\n",
      "Answer:\n",
      "\n",
      "Answer: 0\n",
      "--------------------\n",
      "Expected: -8\n",
      "Model: 0\n",
      "Correct: 14, Total: 72\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (6 + 9) * 3?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (6 + 9) * 3?\n",
      "Answer:\n",
      "\n",
      "Answer: 45\n",
      "--------------------\n",
      "Expected: 45\n",
      "Model: 45\n",
      "Correct: 15, Total: 73\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (3 * 4) * 6?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (3 * 4) * 6?\n",
      "Answer:\n",
      "\n",
      "Answer: 48\n",
      "--------------------\n",
      "Expected: 72\n",
      "Model: 48\n",
      "Correct: 15, Total: 74\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 - 9) * 6?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 - 9) * 6?\n",
      "Answer:\n",
      "\n",
      "Answer: 3\n",
      "--------------------\n",
      "Expected: 0\n",
      "Model: 3\n",
      "Correct: 15, Total: 75\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 - 1) * 3?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 - 1) * 3?\n",
      "Answer:\n",
      "\n",
      "Answer: 12\n",
      "--------------------\n",
      "Expected: 24\n",
      "Model: 12\n",
      "Correct: 15, Total: 76\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (5 + 3) - 6?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (5 + 3) - 6?\n",
      "Answer:\n",
      "\n",
      "Answer: 0\n",
      "--------------------\n",
      "Expected: 2\n",
      "Model: 0\n",
      "Correct: 15, Total: 77\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 + 7) + 9?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 + 7) + 9?\n",
      "Answer:\n",
      "\n",
      "Answer: 22\n",
      "--------------------\n",
      "Expected: 20\n",
      "Model: 22\n",
      "Correct: 15, Total: 78\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 * 2) * 1?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 * 2) * 1?\n",
      "Answer:\n",
      "\n",
      "Answer: 21\n",
      "--------------------\n",
      "Expected: 18\n",
      "Model: 21\n",
      "Correct: 15, Total: 79\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 + 1) + 3?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 + 1) + 3?\n",
      "Answer:\n",
      "\n",
      "Answer: 10\n",
      "--------------------\n",
      "Expected: 11\n",
      "Model: 10\n",
      "Correct: 15, Total: 80\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (6 * 1) * 4?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (6 * 1) * 4?\n",
      "Answer:\n",
      "\n",
      "Answer: 32\n",
      "--------------------\n",
      "Expected: 24\n",
      "Model: 32\n",
      "Correct: 15, Total: 81\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 * 8) - 5?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 * 8) - 5?\n",
      "Answer:\n",
      "\n",
      "Answer: 13\n",
      "--------------------\n",
      "Expected: 59\n",
      "Model: 13\n",
      "Correct: 15, Total: 82\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 + 1) + 4?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 + 1) + 4?\n",
      "Answer:\n",
      "\n",
      "Answer: 13\n",
      "--------------------\n",
      "Expected: 13\n",
      "Model: 13\n",
      "Correct: 16, Total: 83\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 + 5) * 5?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 + 5) * 5?\n",
      "Answer:\n",
      "\n",
      "Answer: 50\n",
      "--------------------\n",
      "Expected: 65\n",
      "Model: 50\n",
      "Correct: 16, Total: 84\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 * 3) - 8?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 * 3) - 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 4\n",
      "Model: 1\n",
      "Correct: 16, Total: 85\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (1 + 9) + 3?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (1 + 9) + 3?\n",
      "Answer:\n",
      "\n",
      "Answer: 14\n",
      "--------------------\n",
      "Expected: 13\n",
      "Model: 14\n",
      "Correct: 16, Total: 86\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 + 2) - 8?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 + 2) - 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 2\n",
      "Model: 1\n",
      "Correct: 16, Total: 87\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 * 7) - 8?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 * 7) - 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 48\n",
      "Model: 1\n",
      "Correct: 16, Total: 88\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 - 7) * 2?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 - 7) * 2?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: -6\n",
      "Model: 1\n",
      "Correct: 16, Total: 89\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (1 + 9) - 6?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (1 + 9) - 6?\n",
      "Answer:\n",
      "\n",
      "Answer: 2\n",
      "--------------------\n",
      "Expected: 4\n",
      "Model: 2\n",
      "Correct: 16, Total: 90\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 + 4) - 9?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 + 4) - 9?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 4\n",
      "Model: 1\n",
      "Correct: 16, Total: 91\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (1 + 6) + 6?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (1 + 6) + 6?\n",
      "Answer:\n",
      "\n",
      "Answer: 13\n",
      "--------------------\n",
      "Expected: 13\n",
      "Model: 13\n",
      "Correct: 17, Total: 92\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (3 * 5) + 8?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (3 * 5) + 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 23\n",
      "--------------------\n",
      "Expected: 23\n",
      "Model: 23\n",
      "Correct: 18, Total: 93\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 * 6) - 9?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 * 6) - 9?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 15\n",
      "Model: 1\n",
      "Correct: 18, Total: 94\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 + 1) + 6?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 + 1) + 6?\n",
      "Answer:\n",
      "\n",
      "Answer: 17\n",
      "--------------------\n",
      "Expected: 15\n",
      "Model: 17\n",
      "Correct: 18, Total: 95\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (2 + 6) + 7?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (2 + 6) + 7?\n",
      "Answer:\n",
      "\n",
      "Answer: 17\n",
      "--------------------\n",
      "Expected: 15\n",
      "Model: 17\n",
      "Correct: 18, Total: 96\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 * 7) * 9?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 * 7) * 9?\n",
      "Answer:\n",
      "\n",
      "Answer: 105\n",
      "--------------------\n",
      "Expected: 567\n",
      "Model: 105\n",
      "Correct: 18, Total: 97\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (2 * 5) + 6?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (2 * 5) + 6?\n",
      "Answer:\n",
      "\n",
      "Answer: 17\n",
      "--------------------\n",
      "Expected: 16\n",
      "Model: 17\n",
      "Correct: 18, Total: 98\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (5 - 3) * 9?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (5 - 3) * 9?\n",
      "Answer:\n",
      "\n",
      "Answer: 27\n",
      "--------------------\n",
      "Expected: 18\n",
      "Model: 27\n",
      "Correct: 18, Total: 99\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (2 - 7) - 4?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (2 - 7) - 4?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: -9\n",
      "Model: 1\n",
      "Correct: 18, Total: 100\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (5 - 1) * 9?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (5 - 1) * 9?\n",
      "Answer:\n",
      "\n",
      "Answer: 33\n",
      "--------------------\n",
      "Expected: 36\n",
      "Model: 33\n",
      "Correct: 18, Total: 101\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 - 8) + 5?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 - 8) + 5?\n",
      "Answer:\n",
      "\n",
      "Answer: 10\n",
      "--------------------\n",
      "Expected: 6\n",
      "Model: 10\n",
      "Correct: 18, Total: 102\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 - 7) - 9?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 - 7) - 9?\n",
      "Answer:\n",
      "\n",
      "Answer: 0\n",
      "--------------------\n",
      "Expected: -9\n",
      "Model: 0\n",
      "Correct: 18, Total: 103\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 + 8) + 3?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 + 8) + 3?\n",
      "Answer:\n",
      "\n",
      "Answer: 21\n",
      "--------------------\n",
      "Expected: 19\n",
      "Model: 21\n",
      "Correct: 18, Total: 104\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 * 9) * 8?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 * 9) * 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 104\n",
      "--------------------\n",
      "Expected: 288\n",
      "Model: 104\n",
      "Correct: 18, Total: 105\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (2 * 8) - 5?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (2 * 8) - 5?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 11\n",
      "Model: 1\n",
      "Correct: 18, Total: 106\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 + 6) + 4?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 + 6) + 4?\n",
      "Answer:\n",
      "\n",
      "Answer: 19\n",
      "--------------------\n",
      "Expected: 17\n",
      "Model: 19\n",
      "Correct: 18, Total: 107\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (5 * 3) + 7?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (5 * 3) + 7?\n",
      "Answer:\n",
      "\n",
      "Answer: 20\n",
      "--------------------\n",
      "Expected: 22\n",
      "Model: 20\n",
      "Correct: 18, Total: 108\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 - 6) + 7?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 - 6) + 7?\n",
      "Answer:\n",
      "\n",
      "Answer: 16\n",
      "--------------------\n",
      "Expected: 10\n",
      "Model: 16\n",
      "Correct: 18, Total: 109\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 * 8) - 8?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 * 8) - 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 16\n",
      "--------------------\n",
      "Expected: 56\n",
      "Model: 16\n",
      "Correct: 18, Total: 110\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 + 9) * 1?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 + 9) * 1?\n",
      "Answer:\n",
      "\n",
      "Answer: 23\n",
      "--------------------\n",
      "Expected: 17\n",
      "Model: 23\n",
      "Correct: 18, Total: 111\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (6 + 2) * 2?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (6 + 2) * 2?\n",
      "Answer:\n",
      "\n",
      "Answer: 14\n",
      "--------------------\n",
      "Expected: 16\n",
      "Model: 14\n",
      "Correct: 18, Total: 112\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (5 - 8) * 3?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (5 - 8) * 3?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: -9\n",
      "Model: 1\n",
      "Correct: 18, Total: 113\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (1 - 2) - 1?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (1 - 2) - 1?\n",
      "Answer:\n",
      "\n",
      "Answer: 0\n",
      "--------------------\n",
      "Expected: -2\n",
      "Model: 0\n",
      "Correct: 18, Total: 114\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (5 * 9) + 2?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (5 * 9) + 2?\n",
      "Answer:\n",
      "\n",
      "Answer: 23\n",
      "--------------------\n",
      "Expected: 47\n",
      "Model: 23\n",
      "Correct: 18, Total: 115\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (2 * 9) - 1?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (2 * 9) - 1?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 17\n",
      "Model: 1\n",
      "Correct: 18, Total: 116\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 * 7) + 7?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 * 7) + 7?\n",
      "Answer:\n",
      "\n",
      "Answer: 40\n",
      "--------------------\n",
      "Expected: 70\n",
      "Model: 40\n",
      "Correct: 18, Total: 117\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 - 4) + 4?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 - 4) + 4?\n",
      "Answer:\n",
      "\n",
      "Answer: 11\n",
      "--------------------\n",
      "Expected: 7\n",
      "Model: 11\n",
      "Correct: 18, Total: 118\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 + 9) + 6?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 + 9) + 6?\n",
      "Answer:\n",
      "\n",
      "Answer: 24\n",
      "--------------------\n",
      "Expected: 22\n",
      "Model: 24\n",
      "Correct: 18, Total: 119\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 + 9) + 8?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 + 9) + 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 25\n",
      "--------------------\n",
      "Expected: 25\n",
      "Model: 25\n",
      "Correct: 19, Total: 120\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (3 - 5) - 9?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (3 - 5) - 9?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: -11\n",
      "Model: 1\n",
      "Correct: 19, Total: 121\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (2 + 5) * 1?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (2 + 5) * 1?\n",
      "Answer:\n",
      "\n",
      "Answer: 7\n",
      "--------------------\n",
      "Expected: 7\n",
      "Model: 7\n",
      "Correct: 20, Total: 122\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (3 + 8) + 7?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (3 + 8) + 7?\n",
      "Answer:\n",
      "\n",
      "Answer: 20\n",
      "--------------------\n",
      "Expected: 18\n",
      "Model: 20\n",
      "Correct: 20, Total: 123\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 * 7) * 4?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 * 7) * 4?\n",
      "Answer:\n",
      "\n",
      "Answer: 40\n",
      "--------------------\n",
      "Expected: 112\n",
      "Model: 40\n",
      "Correct: 20, Total: 124\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 - 9) * 6?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 - 9) * 6?\n",
      "Answer:\n",
      "\n",
      "Answer: 24\n",
      "--------------------\n",
      "Expected: -30\n",
      "Model: 24\n",
      "Correct: 20, Total: 125\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 + 4) - 3?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 + 4) - 3?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 8\n",
      "Model: 1\n",
      "Correct: 20, Total: 126\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 * 5) * 1?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 * 5) * 1?\n",
      "Answer:\n",
      "\n",
      "Answer: 10\n",
      "--------------------\n",
      "Expected: 20\n",
      "Model: 10\n",
      "Correct: 20, Total: 127\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (6 * 3) * 1?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (6 * 3) * 1?\n",
      "Answer:\n",
      "\n",
      "Answer: 18\n",
      "--------------------\n",
      "Expected: 18\n",
      "Model: 18\n",
      "Correct: 21, Total: 128\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (6 * 5) * 3?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (6 * 5) * 3?\n",
      "Answer:\n",
      "\n",
      "Answer: 45\n",
      "--------------------\n",
      "Expected: 90\n",
      "Model: 45\n",
      "Correct: 21, Total: 129\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (3 * 4) + 9?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (3 * 4) + 9?\n",
      "Answer:\n",
      "\n",
      "Answer: 23\n",
      "--------------------\n",
      "Expected: 21\n",
      "Model: 23\n",
      "Correct: 21, Total: 130\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 - 2) - 1?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 - 2) - 1?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 5\n",
      "Model: 1\n",
      "Correct: 21, Total: 131\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 + 9) * 1?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 + 9) * 1?\n",
      "Answer:\n",
      "\n",
      "Answer: 13\n",
      "--------------------\n",
      "Expected: 13\n",
      "Model: 13\n",
      "Correct: 22, Total: 132\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (6 * 5) - 2?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (6 * 5) - 2?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 28\n",
      "Model: 1\n",
      "Correct: 22, Total: 133\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (1 * 9) + 6?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (1 * 9) + 6?\n",
      "Answer:\n",
      "\n",
      "Answer: 15\n",
      "--------------------\n",
      "Expected: 15\n",
      "Model: 15\n",
      "Correct: 23, Total: 134\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 - 2) * 8?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 - 2) * 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 32\n",
      "--------------------\n",
      "Expected: 40\n",
      "Model: 32\n",
      "Correct: 23, Total: 135\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (1 * 4) + 1?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (1 * 4) + 1?\n",
      "Answer:\n",
      "\n",
      "Answer: 5\n",
      "--------------------\n",
      "Expected: 5\n",
      "Model: 5\n",
      "Correct: 24, Total: 136\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 + 7) * 7?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 + 7) * 7?\n",
      "Answer:\n",
      "\n",
      "Answer: 70\n",
      "--------------------\n",
      "Expected: 105\n",
      "Model: 70\n",
      "Correct: 24, Total: 137\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (6 - 4) - 6?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (6 - 4) - 6?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: -4\n",
      "Model: 1\n",
      "Correct: 24, Total: 138\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (3 - 3) + 7?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (3 - 3) + 7?\n",
      "Answer:\n",
      "\n",
      "Answer: 10\n",
      "--------------------\n",
      "Expected: 7\n",
      "Model: 10\n",
      "Correct: 24, Total: 139\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 * 1) - 2?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 * 1) - 2?\n",
      "Answer:\n",
      "\n",
      "Answer: 5\n",
      "--------------------\n",
      "Expected: 7\n",
      "Model: 5\n",
      "Correct: 24, Total: 140\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 + 6) + 4?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 + 6) + 4?\n",
      "Answer:\n",
      "\n",
      "Answer: 21\n",
      "--------------------\n",
      "Expected: 18\n",
      "Model: 21\n",
      "Correct: 24, Total: 141\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 - 1) + 6?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 - 1) + 6?\n",
      "Answer:\n",
      "\n",
      "Answer: 13\n",
      "--------------------\n",
      "Expected: 13\n",
      "Model: 13\n",
      "Correct: 25, Total: 142\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (6 + 3) - 8?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (6 + 3) - 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 1\n",
      "Model: 1\n",
      "Correct: 26, Total: 143\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (6 * 7) - 2?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (6 * 7) - 2?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "What is (6 * 7) - 2?\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 40\n",
      "Model: 1\n",
      "Correct: 26, Total: 144\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (2 - 1) + 4?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (2 - 1) + 4?\n",
      "Answer:\n",
      "\n",
      "Answer: 7\n",
      "--------------------\n",
      "Expected: 5\n",
      "Model: 7\n",
      "Correct: 26, Total: 145\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 * 5) + 6?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 * 5) + 6?\n",
      "Answer:\n",
      "\n",
      "Answer: 21\n",
      "--------------------\n",
      "Expected: 41\n",
      "Model: 21\n",
      "Correct: 26, Total: 146\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 * 7) * 7?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 * 7) * 7?\n",
      "Answer:\n",
      "\n",
      "Answer: 14\n",
      "--------------------\n",
      "Expected: 196\n",
      "Model: 14\n",
      "Correct: 26, Total: 147\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (1 * 5) * 7?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (1 * 5) * 7?\n",
      "Answer:\n",
      "\n",
      "Answer: 35\n",
      "--------------------\n",
      "Expected: 35\n",
      "Model: 35\n",
      "Correct: 27, Total: 148\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 + 9) * 2?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 + 9) * 2?\n",
      "Answer:\n",
      "\n",
      "Answer: 34\n",
      "--------------------\n",
      "Expected: 36\n",
      "Model: 34\n",
      "Correct: 27, Total: 149\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 + 1) + 5?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 + 1) + 5?\n",
      "Answer:\n",
      "\n",
      "Answer: 13\n",
      "--------------------\n",
      "Expected: 13\n",
      "Model: 13\n",
      "Correct: 28, Total: 150\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (3 - 1) - 9?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (3 - 1) - 9?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: -7\n",
      "Model: 1\n",
      "Correct: 28, Total: 151\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (5 - 6) - 2?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (5 - 6) - 2?\n",
      "Answer:\n",
      "\n",
      "Answer: 0\n",
      "--------------------\n",
      "Expected: -3\n",
      "Model: 0\n",
      "Correct: 28, Total: 152\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 - 1) * 4?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 - 1) * 4?\n",
      "Answer:\n",
      "\n",
      "Answer: 20\n",
      "--------------------\n",
      "Expected: 28\n",
      "Model: 20\n",
      "Correct: 28, Total: 153\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (3 - 8) + 6?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (3 - 8) + 6?\n",
      "Answer:\n",
      "\n",
      "Answer: 11\n",
      "--------------------\n",
      "Expected: 1\n",
      "Model: 11\n",
      "Correct: 28, Total: 154\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 - 3) + 7?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 - 3) + 7?\n",
      "Answer:\n",
      "\n",
      "Answer: 16\n",
      "--------------------\n",
      "Expected: 13\n",
      "Model: 16\n",
      "Correct: 28, Total: 155\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 - 9) - 1?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 - 9) - 1?\n",
      "Answer:\n",
      "\n",
      "Answer: 0\n",
      "--------------------\n",
      "Expected: -2\n",
      "Model: 0\n",
      "Correct: 28, Total: 156\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 - 6) * 7?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 - 6) * 7?\n",
      "Answer:\n",
      "\n",
      "Answer: 33\n",
      "--------------------\n",
      "Expected: 7\n",
      "Model: 33\n",
      "Correct: 28, Total: 157\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (5 * 2) * 7?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (5 * 2) * 7?\n",
      "Answer:\n",
      "\n",
      "Answer: 42\n",
      "--------------------\n",
      "Expected: 70\n",
      "Model: 42\n",
      "Correct: 28, Total: 158\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 * 1) - 8?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 * 1) - 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: -4\n",
      "Model: 1\n",
      "Correct: 28, Total: 159\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 + 8) + 8?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 + 8) + 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 30\n",
      "--------------------\n",
      "Expected: 24\n",
      "Model: 30\n",
      "Correct: 28, Total: 160\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 * 8) - 1?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 * 8) - 1?\n",
      "Answer:\n",
      "\n",
      "Answer: 13\n",
      "--------------------\n",
      "Expected: 55\n",
      "Model: 13\n",
      "Correct: 28, Total: 161\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (1 * 6) - 5?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (1 * 6) - 5?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 1\n",
      "Model: 1\n",
      "Correct: 29, Total: 162\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (6 - 4) * 8?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (6 - 4) * 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 40\n",
      "--------------------\n",
      "Expected: 16\n",
      "Model: 40\n",
      "Correct: 29, Total: 163\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (1 * 6) * 3?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (1 * 6) * 3?\n",
      "Answer:\n",
      "\n",
      "Answer: 18\n",
      "--------------------\n",
      "Expected: 18\n",
      "Model: 18\n",
      "Correct: 30, Total: 164\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (3 - 2) - 9?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (3 - 2) - 9?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: -8\n",
      "Model: 1\n",
      "Correct: 30, Total: 165\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (3 + 6) - 1?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (3 + 6) - 1?\n",
      "Answer:\n",
      "\n",
      "Answer: 0\n",
      "--------------------\n",
      "Expected: 8\n",
      "Model: 0\n",
      "Correct: 30, Total: 166\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 * 3) * 9?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 * 3) * 9?\n",
      "Answer:\n",
      "\n",
      "Answer: 81\n",
      "--------------------\n",
      "Expected: 243\n",
      "Model: 81\n",
      "Correct: 30, Total: 167\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (6 - 7) * 5?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (6 - 7) * 5?\n",
      "Answer:\n",
      "\n",
      "Answer: 15\n",
      "--------------------\n",
      "Expected: -5\n",
      "Model: 15\n",
      "Correct: 30, Total: 168\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (6 - 5) + 9?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (6 - 5) + 9?\n",
      "Answer:\n",
      "\n",
      "Answer: 16\n",
      "--------------------\n",
      "Expected: 10\n",
      "Model: 16\n",
      "Correct: 30, Total: 169\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (6 * 5) + 7?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (6 * 5) + 7?\n",
      "Answer:\n",
      "\n",
      "Answer: 22\n",
      "--------------------\n",
      "Expected: 37\n",
      "Model: 22\n",
      "Correct: 30, Total: 170\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (3 * 8) - 1?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (3 * 8) - 1?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 23\n",
      "Model: 1\n",
      "Correct: 30, Total: 171\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 * 9) - 3?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 * 9) - 3?\n",
      "Answer:\n",
      "\n",
      "Answer: 24\n",
      "--------------------\n",
      "Expected: 78\n",
      "Model: 24\n",
      "Correct: 30, Total: 172\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (3 * 6) + 2?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (3 * 6) + 2?\n",
      "Answer:\n",
      "\n",
      "Answer: 10\n",
      "--------------------\n",
      "Expected: 20\n",
      "Model: 10\n",
      "Correct: 30, Total: 173\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (2 - 2) - 9?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (2 - 2) - 9?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: -9\n",
      "Model: 1\n",
      "Correct: 30, Total: 174\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 + 9) * 4?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 + 9) * 4?\n",
      "Answer:\n",
      "\n",
      "Answer: 54\n",
      "--------------------\n",
      "Expected: 72\n",
      "Model: 54\n",
      "Correct: 30, Total: 175\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 * 4) * 4?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 * 4) * 4?\n",
      "Answer:\n",
      "\n",
      "Answer: 44\n",
      "--------------------\n",
      "Expected: 64\n",
      "Model: 44\n",
      "Correct: 30, Total: 176\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 - 2) * 9?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 - 2) * 9?\n",
      "Answer:\n",
      "\n",
      "Answer: 45\n",
      "--------------------\n",
      "Expected: 63\n",
      "Model: 45\n",
      "Correct: 30, Total: 177\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (2 * 2) * 7?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (2 * 2) * 7?\n",
      "Answer:\n",
      "\n",
      "Answer: 42\n",
      "--------------------\n",
      "Expected: 28\n",
      "Model: 42\n",
      "Correct: 30, Total: 178\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (1 * 6) - 7?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (1 * 6) - 7?\n",
      "Answer:\n",
      "\n",
      "Answer: 3\n",
      "--------------------\n",
      "Expected: -1\n",
      "Model: 3\n",
      "Correct: 30, Total: 179\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (5 - 4) * 8?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (5 - 4) * 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 24\n",
      "--------------------\n",
      "Expected: 8\n",
      "Model: 24\n",
      "Correct: 30, Total: 180\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (5 * 2) - 1?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (5 * 2) - 1?\n",
      "Answer:\n",
      "\n",
      "Answer: 3\n",
      "--------------------\n",
      "Expected: 9\n",
      "Model: 3\n",
      "Correct: 30, Total: 181\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (1 + 6) + 8?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (1 + 6) + 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 21\n",
      "--------------------\n",
      "Expected: 15\n",
      "Model: 21\n",
      "Correct: 30, Total: 182\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 - 4) + 9?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 - 4) + 9?\n",
      "Answer:\n",
      "\n",
      "Answer: 20\n",
      "--------------------\n",
      "Expected: 14\n",
      "Model: 20\n",
      "Correct: 30, Total: 183\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 + 5) - 6?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 + 5) - 6?\n",
      "Answer:\n",
      "\n",
      "Answer: 2\n",
      "--------------------\n",
      "Expected: 8\n",
      "Model: 2\n",
      "Correct: 30, Total: 184\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (5 - 4) * 3?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (5 - 4) * 3?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 3\n",
      "Model: 1\n",
      "Correct: 30, Total: 185\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 * 5) + 3?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 * 5) + 3?\n",
      "Answer:\n",
      "\n",
      "Answer: 16\n",
      "--------------------\n",
      "Expected: 38\n",
      "Model: 16\n",
      "Correct: 30, Total: 186\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 * 2) * 5?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 * 2) * 5?\n",
      "Answer:\n",
      "\n",
      "Answer: 45\n",
      "--------------------\n",
      "Expected: 70\n",
      "Model: 45\n",
      "Correct: 30, Total: 187\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (5 * 1) * 8?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (5 * 1) * 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 40\n",
      "--------------------\n",
      "Expected: 40\n",
      "Model: 40\n",
      "Correct: 31, Total: 188\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 * 2) * 4?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 * 2) * 4?\n",
      "Answer:\n",
      "\n",
      "Answer: 44\n",
      "--------------------\n",
      "Expected: 56\n",
      "Model: 44\n",
      "Correct: 31, Total: 189\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (1 + 7) + 4?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (1 + 7) + 4?\n",
      "Answer:\n",
      "\n",
      "Answer: 12\n",
      "--------------------\n",
      "Expected: 12\n",
      "Model: 12\n",
      "Correct: 32, Total: 190\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (2 + 6) + 4?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (2 + 6) + 4?\n",
      "Answer:\n",
      "\n",
      "Answer: 14\n",
      "--------------------\n",
      "Expected: 12\n",
      "Model: 14\n",
      "Correct: 32, Total: 191\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 + 8) + 2?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 + 8) + 2?\n",
      "Answer:\n",
      "\n",
      "Answer: 14\n",
      "--------------------\n",
      "Expected: 14\n",
      "Model: 14\n",
      "Correct: 33, Total: 192\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (8 * 9) * 9?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (8 * 9) * 9?\n",
      "Answer:\n",
      "\n",
      "Answer: 108\n",
      "--------------------\n",
      "Expected: 648\n",
      "Model: 108\n",
      "Correct: 33, Total: 193\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 + 1) + 8?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 + 1) + 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 21\n",
      "--------------------\n",
      "Expected: 16\n",
      "Model: 21\n",
      "Correct: 33, Total: 194\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 + 2) + 2?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 + 2) + 2?\n",
      "Answer:\n",
      "\n",
      "Answer: 10\n",
      "--------------------\n",
      "Expected: 8\n",
      "Model: 10\n",
      "Correct: 33, Total: 195\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (7 - 4) * 2?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (7 - 4) * 2?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 6\n",
      "Model: 1\n",
      "Correct: 33, Total: 196\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (4 - 2) + 8?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (4 - 2) + 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 13\n",
      "--------------------\n",
      "Expected: 10\n",
      "Model: 13\n",
      "Correct: 33, Total: 197\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (3 * 5) - 8?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (3 * 5) - 8?\n",
      "Answer:\n",
      "\n",
      "Answer: 1\n",
      "--------------------\n",
      "Expected: 7\n",
      "Model: 1\n",
      "Correct: 33, Total: 198\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (1 - 5) - 3?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (1 - 5) - 3?\n",
      "Answer:\n",
      "\n",
      "Answer: 0\n",
      "--------------------\n",
      "Expected: -7\n",
      "Model: 0\n",
      "Correct: 33, Total: 199\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is (9 - 5) * 1?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is (9 - 5) * 1?\n",
      "Answer:\n",
      "\n",
      "Answer: 4\n",
      "--------------------\n",
      "Expected: 4\n",
      "Model: 4\n",
      "Correct: 34, Total: 200\n",
      "\n",
      "\n",
      "Results saved to: results\\Qwen_Qwen2.5-Math-1.5B\\Qwen_Qwen2.5-Math-1.5B_arithmetic_arithmetic_1dc_20250512_150123.csv\n",
      "Type statistics saved to: results\\Qwen_Qwen2.5-Math-1.5B\\Qwen_Qwen2.5-Math-1.5B_arithmetic_arithmetic_1dc_20250512_150123_stats.csv\n",
      "Math Evaluation Results for arithmetic_1dc: {'accuracy': 0.17, 'correct': 34, 'total': 200}\n",
      "Evaluating math performance on dataset arithmetic_2da\n",
      "Evaluating math performance...\n",
      "------\n",
      "Question: What is 67 plus 88?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 67 plus 88?\n",
      "Answer:\n",
      "\n",
      "Answer: 155\n",
      "--------------------\n",
      "Expected: 155\n",
      "Model: 155\n",
      "Correct: 1, Total: 1\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 38 plus 98?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 38 plus 98?\n",
      "Answer:\n",
      "\n",
      "Answer: 196\n",
      "--------------------\n",
      "Expected: 136\n",
      "Model: 196\n",
      "Correct: 1, Total: 2\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 82 plus 64?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 82 plus 64?\n",
      "Answer:\n",
      "\n",
      "Answer: 146\n",
      "--------------------\n",
      "Expected: 146\n",
      "Model: 146\n",
      "Correct: 2, Total: 3\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 12 plus 63?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 12 plus 63?\n",
      "Answer:\n",
      "\n",
      "Answer: 75\n",
      "--------------------\n",
      "Expected: 75\n",
      "Model: 75\n",
      "Correct: 3, Total: 4\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 60 plus 44?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 60 plus 44?\n",
      "Answer:\n",
      "\n",
      "Answer: 104\n",
      "--------------------\n",
      "Expected: 104\n",
      "Model: 104\n",
      "Correct: 4, Total: 5\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 17 plus 99?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 17 plus 99?\n",
      "Answer:\n",
      "\n",
      "Answer: 116\n",
      "--------------------\n",
      "Expected: 116\n",
      "Model: 116\n",
      "Correct: 5, Total: 6\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 54 plus 57?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 54 plus 57?\n",
      "Answer:\n",
      "\n",
      "Answer: 111\n",
      "--------------------\n",
      "Expected: 111\n",
      "Model: 111\n",
      "Correct: 6, Total: 7\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 3 plus 27?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 3 plus 27?\n",
      "Answer:\n",
      "\n",
      "Answer: 30\n",
      "--------------------\n",
      "Expected: 30\n",
      "Model: 30\n",
      "Correct: 7, Total: 8\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 61 plus 94?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 61 plus 94?\n",
      "Answer:\n",
      "\n",
      "Answer: 155\n",
      "--------------------\n",
      "Expected: 155\n",
      "Model: 155\n",
      "Correct: 8, Total: 9\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 14 plus 30?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 14 plus 30?\n",
      "Answer:\n",
      "\n",
      "Answer: 44\n",
      "--------------------\n",
      "Expected: 44\n",
      "Model: 44\n",
      "Correct: 9, Total: 10\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 45 plus 40?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 45 plus 40?\n",
      "Answer:\n",
      "\n",
      "Answer: 85\n",
      "--------------------\n",
      "Expected: 85\n",
      "Model: 85\n",
      "Correct: 10, Total: 11\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 6 plus 39?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 6 plus 39?\n",
      "Answer:\n",
      "\n",
      "Answer: 45\n",
      "--------------------\n",
      "Expected: 45\n",
      "Model: 45\n",
      "Correct: 11, Total: 12\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 55 plus 7?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 55 plus 7?\n",
      "Answer:\n",
      "\n",
      "Answer: 62\n",
      "--------------------\n",
      "Expected: 62\n",
      "Model: 62\n",
      "Correct: 12, Total: 13\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 21 plus 74?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 21 plus 74?\n",
      "Answer:\n",
      "\n",
      "Answer: 95\n",
      "--------------------\n",
      "Expected: 95\n",
      "Model: 95\n",
      "Correct: 13, Total: 14\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 17 plus 48?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 17 plus 48?\n",
      "Answer:\n",
      "\n",
      "Answer: 65\n",
      "--------------------\n",
      "Expected: 65\n",
      "Model: 65\n",
      "Correct: 14, Total: 15\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 64 plus 29?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 64 plus 29?\n",
      "Answer:\n",
      "\n",
      "Answer: 93\n",
      "--------------------\n",
      "Expected: 93\n",
      "Model: 93\n",
      "Correct: 15, Total: 16\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 65 plus 63?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 65 plus 63?\n",
      "Answer:\n",
      "\n",
      "Answer: 138\n",
      "--------------------\n",
      "Expected: 128\n",
      "Model: 138\n",
      "Correct: 15, Total: 17\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 50 plus 6?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 50 plus 6?\n",
      "Answer:\n",
      "\n",
      "Answer: 56\n",
      "--------------------\n",
      "Expected: 56\n",
      "Model: 56\n",
      "Correct: 16, Total: 18\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 5 plus 34?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 5 plus 34?\n",
      "Answer:\n",
      "\n",
      "Answer: 39\n",
      "--------------------\n",
      "Expected: 39\n",
      "Model: 39\n",
      "Correct: 17, Total: 19\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 31 plus 94?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 31 plus 94?\n",
      "Answer:\n",
      "\n",
      "Answer: 125\n",
      "--------------------\n",
      "Expected: 125\n",
      "Model: 125\n",
      "Correct: 18, Total: 20\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 98 plus 50?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 98 plus 50?\n",
      "Answer:\n",
      "\n",
      "Answer: 148\n",
      "--------------------\n",
      "Expected: 148\n",
      "Model: 148\n",
      "Correct: 19, Total: 21\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 17 plus 71?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 17 plus 71?\n",
      "Answer:\n",
      "\n",
      "Answer: 88\n",
      "--------------------\n",
      "Expected: 88\n",
      "Model: 88\n",
      "Correct: 20, Total: 22\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 27 plus 94?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 27 plus 94?\n",
      "Answer:\n",
      "\n",
      "Answer: 121\n",
      "--------------------\n",
      "Expected: 121\n",
      "Model: 121\n",
      "Correct: 21, Total: 23\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 66 plus 99?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 66 plus 99?\n",
      "Answer:\n",
      "\n",
      "Answer: 165\n",
      "--------------------\n",
      "Expected: 165\n",
      "Model: 165\n",
      "Correct: 22, Total: 24\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 36 plus 25?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 36 plus 25?\n",
      "Answer:\n",
      "\n",
      "Answer: 51\n",
      "--------------------\n",
      "Expected: 61\n",
      "Model: 51\n",
      "Correct: 22, Total: 25\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 45 plus 10?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 45 plus 10?\n",
      "Answer:\n",
      "\n",
      "Answer: 55\n",
      "--------------------\n",
      "Expected: 55\n",
      "Model: 55\n",
      "Correct: 23, Total: 26\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 54 plus 77?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 54 plus 77?\n",
      "Answer:\n",
      "\n",
      "Answer: 131\n",
      "--------------------\n",
      "Expected: 131\n",
      "Model: 131\n",
      "Correct: 24, Total: 27\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 86 plus 40?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 86 plus 40?\n",
      "Answer:\n",
      "\n",
      "Answer: 126\n",
      "--------------------\n",
      "Expected: 126\n",
      "Model: 126\n",
      "Correct: 25, Total: 28\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 91 plus 31?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 91 plus 31?\n",
      "Answer:\n",
      "\n",
      "Answer: 122\n",
      "--------------------\n",
      "Expected: 122\n",
      "Model: 122\n",
      "Correct: 26, Total: 29\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 71 plus 49?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 71 plus 49?\n",
      "Answer:\n",
      "\n",
      "Answer: 120\n",
      "--------------------\n",
      "Expected: 120\n",
      "Model: 120\n",
      "Correct: 27, Total: 30\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 10 plus 95?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 10 plus 95?\n",
      "Answer:\n",
      "\n",
      "Answer: 10 + 95 = 105\n",
      "--------------------\n",
      "Expected: 105\n",
      "Model: 105\n",
      "Correct: 28, Total: 31\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 31 plus 38?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 31 plus 38?\n",
      "Answer:\n",
      "\n",
      "Answer: 59\n",
      "--------------------\n",
      "Expected: 69\n",
      "Model: 59\n",
      "Correct: 28, Total: 32\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 84 plus 98?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 84 plus 98?\n",
      "Answer:\n",
      "\n",
      "Answer: 182\n",
      "--------------------\n",
      "Expected: 182\n",
      "Model: 182\n",
      "Correct: 29, Total: 33\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 79 plus 2?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 79 plus 2?\n",
      "Answer:\n",
      "\n",
      "Answer: 81\n",
      "--------------------\n",
      "Expected: 81\n",
      "Model: 81\n",
      "Correct: 30, Total: 34\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 70 plus 33?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 70 plus 33?\n",
      "Answer:\n",
      "\n",
      "Answer: 103\n",
      "--------------------\n",
      "Expected: 103\n",
      "Model: 103\n",
      "Correct: 31, Total: 35\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 4 plus 36?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 4 plus 36?\n",
      "Answer:\n",
      "\n",
      "Answer: 43\n",
      "--------------------\n",
      "Expected: 40\n",
      "Model: 43\n",
      "Correct: 31, Total: 36\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 46 plus 14?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 46 plus 14?\n",
      "Answer:\n",
      "\n",
      "Answer: 50\n",
      "--------------------\n",
      "Expected: 60\n",
      "Model: 50\n",
      "Correct: 31, Total: 37\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 10 plus 69?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 10 plus 69?\n",
      "Answer:\n",
      "\n",
      "Answer: 79\n",
      "--------------------\n",
      "Expected: 79\n",
      "Model: 79\n",
      "Correct: 32, Total: 38\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 40 plus 91?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 40 plus 91?\n",
      "Answer:\n",
      "\n",
      "Answer: 131\n",
      "--------------------\n",
      "Expected: 131\n",
      "Model: 131\n",
      "Correct: 33, Total: 39\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 65 plus 59?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 65 plus 59?\n",
      "Answer:\n",
      "\n",
      "Answer: 124\n",
      "--------------------\n",
      "Expected: 124\n",
      "Model: 124\n",
      "Correct: 34, Total: 40\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 85 plus 80?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 85 plus 80?\n",
      "Answer:\n",
      "\n",
      "Answer: 165\n",
      "--------------------\n",
      "Expected: 165\n",
      "Model: 165\n",
      "Correct: 35, Total: 41\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 21 plus 32?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 21 plus 32?\n",
      "Answer:\n",
      "\n",
      "Answer: 53\n",
      "--------------------\n",
      "Expected: 53\n",
      "Model: 53\n",
      "Correct: 36, Total: 42\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 54 plus 23?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 54 plus 23?\n",
      "Answer:\n",
      "\n",
      "Answer: 97\n",
      "--------------------\n",
      "Expected: 77\n",
      "Model: 97\n",
      "Correct: 36, Total: 43\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 47 plus 43?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 47 plus 43?\n",
      "Answer:\n",
      "\n",
      "Answer: 70\n",
      "--------------------\n",
      "Expected: 90\n",
      "Model: 70\n",
      "Correct: 36, Total: 44\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 56 plus 90?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 56 plus 90?\n",
      "Answer:\n",
      "\n",
      "Answer: 146\n",
      "--------------------\n",
      "Expected: 146\n",
      "Model: 146\n",
      "Correct: 37, Total: 45\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 9 plus 17?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 9 plus 17?\n",
      "Answer:\n",
      "\n",
      "Answer: 26\n",
      "--------------------\n",
      "Expected: 26\n",
      "Model: 26\n",
      "Correct: 38, Total: 46\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 53 plus 56?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 53 plus 56?\n",
      "Answer:\n",
      "\n",
      "Answer: 109\n",
      "--------------------\n",
      "Expected: 109\n",
      "Model: 109\n",
      "Correct: 39, Total: 47\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 47 plus 62?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 47 plus 62?\n",
      "Answer:\n",
      "\n",
      "Answer: 119\n",
      "--------------------\n",
      "Expected: 109\n",
      "Model: 119\n",
      "Correct: 39, Total: 48\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 58 plus 23?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 58 plus 23?\n",
      "Answer:\n",
      "\n",
      "Answer: 81\n",
      "--------------------\n",
      "Expected: 81\n",
      "Model: 81\n",
      "Correct: 40, Total: 49\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 70 plus 85?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 70 plus 85?\n",
      "Answer:\n",
      "\n",
      "Answer: 155\n",
      "--------------------\n",
      "Expected: 155\n",
      "Model: 155\n",
      "Correct: 41, Total: 50\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 90 plus 2?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 90 plus 2?\n",
      "Answer:\n",
      "\n",
      "Answer: 92\n",
      "--------------------\n",
      "Expected: 92\n",
      "Model: 92\n",
      "Correct: 42, Total: 51\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 55 plus 60?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 55 plus 60?\n",
      "Answer:\n",
      "\n",
      "Answer: 115\n",
      "--------------------\n",
      "Expected: 115\n",
      "Model: 115\n",
      "Correct: 43, Total: 52\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 34 plus 36?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 34 plus 36?\n",
      "Answer:\n",
      "\n",
      "Answer: 50\n",
      "--------------------\n",
      "Expected: 70\n",
      "Model: 50\n",
      "Correct: 43, Total: 53\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 66 plus 81?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 66 plus 81?\n",
      "Answer:\n",
      "\n",
      "Answer: 147\n",
      "--------------------\n",
      "Expected: 147\n",
      "Model: 147\n",
      "Correct: 44, Total: 54\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 66 plus 85?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 66 plus 85?\n",
      "Answer:\n",
      "\n",
      "Answer: 151\n",
      "--------------------\n",
      "Expected: 151\n",
      "Model: 151\n",
      "Correct: 45, Total: 55\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 97 plus 55?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 97 plus 55?\n",
      "Answer:\n",
      "\n",
      "Answer: 152\n",
      "--------------------\n",
      "Expected: 152\n",
      "Model: 152\n",
      "Correct: 46, Total: 56\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 83 plus 60?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 83 plus 60?\n",
      "Answer:\n",
      "\n",
      "Answer: 143\n",
      "--------------------\n",
      "Expected: 143\n",
      "Model: 143\n",
      "Correct: 47, Total: 57\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 2 plus 6?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 2 plus 6?\n",
      "Answer:\n",
      "\n",
      "Answer: 8\n",
      "--------------------\n",
      "Expected: 8\n",
      "Model: 8\n",
      "Correct: 48, Total: 58\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 95 plus 96?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 95 plus 96?\n",
      "Answer:\n",
      "\n",
      "Answer: 191\n",
      "--------------------\n",
      "Expected: 191\n",
      "Model: 191\n",
      "Correct: 49, Total: 59\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 41 plus 15?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 41 plus 15?\n",
      "Answer:\n",
      "\n",
      "Answer: 41 + 15 = 56\n",
      "--------------------\n",
      "Expected: 56\n",
      "Model: 56\n",
      "Correct: 50, Total: 60\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 53 plus 65?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 53 plus 65?\n",
      "Answer:\n",
      "\n",
      "Answer: 98\n",
      "--------------------\n",
      "Expected: 118\n",
      "Model: 98\n",
      "Correct: 50, Total: 61\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 36 plus 37?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 36 plus 37?\n",
      "Answer:\n",
      "\n",
      "Answer: 53\n",
      "--------------------\n",
      "Expected: 73\n",
      "Model: 53\n",
      "Correct: 50, Total: 62\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 62 plus 44?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 62 plus 44?\n",
      "Answer:\n",
      "\n",
      "Answer: 106\n",
      "--------------------\n",
      "Expected: 106\n",
      "Model: 106\n",
      "Correct: 51, Total: 63\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 35 plus 40?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 35 plus 40?\n",
      "Answer:\n",
      "\n",
      "Answer: 75\n",
      "--------------------\n",
      "Expected: 75\n",
      "Model: 75\n",
      "Correct: 52, Total: 64\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 69 plus 12?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 69 plus 12?\n",
      "Answer:\n",
      "\n",
      "Answer: 81\n",
      "--------------------\n",
      "Expected: 81\n",
      "Model: 81\n",
      "Correct: 53, Total: 65\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 81 plus 39?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 81 plus 39?\n",
      "Answer:\n",
      "\n",
      "Answer: 120\n",
      "--------------------\n",
      "Expected: 120\n",
      "Model: 120\n",
      "Correct: 54, Total: 66\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 96 plus 30?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 96 plus 30?\n",
      "Answer:\n",
      "\n",
      "Answer: 126\n",
      "--------------------\n",
      "Expected: 126\n",
      "Model: 126\n",
      "Correct: 55, Total: 67\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 57 plus 56?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 57 plus 56?\n",
      "Answer:\n",
      "\n",
      "Answer: 113\n",
      "--------------------\n",
      "Expected: 113\n",
      "Model: 113\n",
      "Correct: 56, Total: 68\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 67 plus 75?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 67 plus 75?\n",
      "Answer:\n",
      "\n",
      "Answer: 142\n",
      "--------------------\n",
      "Expected: 142\n",
      "Model: 142\n",
      "Correct: 57, Total: 69\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 44 plus 17?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 44 plus 17?\n",
      "Answer:\n",
      "\n",
      "Answer: 44 + 17 = 61\n",
      "--------------------\n",
      "Expected: 61\n",
      "Model: 61\n",
      "Correct: 58, Total: 70\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 75 plus 37?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 75 plus 37?\n",
      "Answer:\n",
      "\n",
      "Answer: 112\n",
      "--------------------\n",
      "Expected: 112\n",
      "Model: 112\n",
      "Correct: 59, Total: 71\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 23 plus 69?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 23 plus 69?\n",
      "Answer:\n",
      "\n",
      "Answer: 92\n",
      "--------------------\n",
      "Expected: 92\n",
      "Model: 92\n",
      "Correct: 60, Total: 72\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 85 plus 16?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 85 plus 16?\n",
      "Answer:\n",
      "\n",
      "Answer: 101\n",
      "--------------------\n",
      "Expected: 101\n",
      "Model: 101\n",
      "Correct: 61, Total: 73\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 7 plus 1?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 7 plus 1?\n",
      "Answer:\n",
      "\n",
      "Answer: 8\n",
      "--------------------\n",
      "Expected: 8\n",
      "Model: 8\n",
      "Correct: 62, Total: 74\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 33 plus 85?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 33 plus 85?\n",
      "Answer:\n",
      "\n",
      "Answer: 128\n",
      "--------------------\n",
      "Expected: 118\n",
      "Model: 128\n",
      "Correct: 62, Total: 75\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 53 plus 48?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 53 plus 48?\n",
      "Answer:\n",
      "\n",
      "Answer: 101\n",
      "--------------------\n",
      "Expected: 101\n",
      "Model: 101\n",
      "Correct: 63, Total: 76\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 20 plus 0?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 20 plus 0?\n",
      "Answer:\n",
      "\n",
      "Answer: 20\n",
      "--------------------\n",
      "Expected: 20\n",
      "Model: 20\n",
      "Correct: 64, Total: 77\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 0 plus 6?\n",
      "Answer:\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 0 plus 6?\n",
      "Answer:\n",
      "\n",
      "Answer: 6\n",
      "--------------------\n",
      "Expected: 6\n",
      "Model: 6\n",
      "Correct: 65, Total: 78\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 95 plus 53?\n",
      "Answer:\n",
      "------\n",
      "USING TOOLS, PRE-TOOL TEXT: Question: What is 95 plus 53?\n",
      "Answer:\n",
      "\n",
      "Answer: 148\n",
      "--------------------\n",
      "Expected: 148\n",
      "Model: 148\n",
      "Correct: 66, Total: 79\n",
      "\n",
      "\n",
      "------\n",
      "Question: What is 93 plus 27?\n",
      "Answer:\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "print_section(\"Latest Checkpoint Evaluation\")\n",
    "model, metadata = load_model(os.path.join(os.curdir, \"toolformer_model\", \"checkpoint-225\"))\n",
    "print_section(\"Most recent training Model Evaluation\")\n",
    "eval_model(MODEL_NAME, DATASET, test_data, model, tokenizer, use_tool=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2b250a",
   "metadata": {},
   "source": [
    "# Pure Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5334a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PURE FINE TUNING TRAINING\n",
    "print_section(\"Pure Fine Tuning Training\")\n",
    "\n",
    "# Prepare the training data based on dataset type\n",
    "train_dataset = combine_and_tokenize(train_data, tokenizer, path=PURE_TRAIN_DATASET_PATH)\n",
    "\n",
    "# Load previous model if it exists\n",
    "try:\n",
    "    previous_path = os.path.join(CHECKPOINTS, \"finetuned\", TOOL_FINETUNED_SAVE_PATH)\n",
    "    tokenizer, model, metadata = load_model(previous_path)\n",
    "    \n",
    "    # Get total epochs from metadata\n",
    "    total_epochs = metadata.get(\"total_epochs\", 0) + current_epochs_pure\n",
    "    print(f\"Continuing training from {metadata.get('total_epochs', 0)} epochs to {total_epochs} epochs\")\n",
    "except FileNotFoundError:\n",
    "    # Start fresh training\n",
    "    total_epochs = current_epochs_pure\n",
    "    print(f\"Starting fresh training for {current_epochs_pure} epochs\")\n",
    "\n",
    "# Train the model\n",
    "model, tokenizer, metadata = train_model(model, tokenizer, train_dataset, num_epochs=current_epochs_pure)\n",
    "\n",
    "# Save with updated epoch count\n",
    "saved_path = save_model(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    os.path.join(CHECKPOINTS, \"finetuned\", TOOL_FINETUNED_SAVE_PATH),\n",
    "    epochs=current_epochs_pure,\n",
    "    total_epochs=total_epochs\n",
    ")\n",
    "print(f\"Saved fine-tuned model to {saved_path} (Total epochs: {total_epochs})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752e89c1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
