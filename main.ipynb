{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "116a82f1",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e772023f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from model.setup import setup_model, save_model, load_model, load_tokenizer\n",
    "from utils.console import isYes, printc, inputc, print_section\n",
    "from model.training import preprocess_for_training, train_model\n",
    "from model.custom_training import train_toolformer_model\n",
    "from data.gsm8k import prepare_gsm8k_dataset\n",
    "from data.svamp import prepare_svamp_dataset\n",
    "from data.arithmetic import prepare_arithmetic_datasets\n",
    "from evaluation.math_evaluation import evaluate_math_performance\n",
    "from evaluation.eval_pipeline import eval_model\n",
    "from constants import MODEL_NAME, INITIAL_SAVE_PATH, TOOL_FINETUNED_SAVE_PATH, DATASET, CHECKPOINTS, TOOL_TRAIN_DATASET_PATH, PURE_TRAIN_DATASET_PATH, EVAL_DATASET_PATH\n",
    "from data.arithmetic import combine_and_tokenize\n",
    "import wandb\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1ea1fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "current_epochs_tool = 1\n",
    "current_epochs_pure = 1\n",
    "data_points = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3516b64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[032m==================================================\u001b[0m\n",
      "\u001b[032m\n",
      "Loading Model\n",
      "\u001b[0m\n",
      "\u001b[032m==================================================\u001b[0m\n",
      "Loading model from local path: ./checkpoints\\pretrained\\qwen-initial\n",
      "Loaded model from saved path\n",
      "\u001b[032m==================================================\u001b[0m\n",
      "\u001b[032m\n",
      "Adding Tool Tokens\n",
      "\u001b[0m\n",
      "\u001b[032m==================================================\u001b[0m\n",
      "Added 2 special tokens to the tokenizer\n",
      "Resized model embeddings to 151648 tokens\n",
      "Special tokens: ['<|endoftext|>', '<tool:calculator>', '</tool>']\n"
     ]
    }
   ],
   "source": [
    "print_section(\"Loading Model\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# Try to load from saved path first, if it fails, download from HF\n",
    "try:\n",
    "    model, metadata = load_model(os.path.join(CHECKPOINTS, \"pretrained\", INITIAL_SAVE_PATH))\n",
    "    tokenizer = load_tokenizer(os.path.join(CHECKPOINTS, \"pretrained\", INITIAL_SAVE_PATH))\n",
    "    print(\"Loaded model from saved path\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Initial model not found. Setting up from {MODEL_NAME}\")\n",
    "    tokenizer, model, metadata = setup_model(MODEL_NAME)\n",
    "    save_model(model, tokenizer, os.path.join(CHECKPOINTS, \"pretrained\", INITIAL_SAVE_PATH))\n",
    "\n",
    "print_section(\"Adding Tool Tokens\")\n",
    "tool_tokens = {\n",
    "    \"additional_special_tokens\": [\n",
    "        \"<tool:calculator>\",\n",
    "        \"</tool>\",\n",
    "    ]\n",
    "}\n",
    "num_added = tokenizer.add_special_tokens(tool_tokens)\n",
    "print(f\"Added {num_added} special tokens to the tokenizer\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"Resized model embeddings to {len(tokenizer)} tokens\")\n",
    "print(\"Special tokens:\", tokenizer.all_special_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a320640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[032m==================================================\u001b[0m\n",
      "\u001b[032m\n",
      "Loading Data\n",
      "\u001b[0m\n",
      "\u001b[032m==================================================\u001b[0m\n",
      "Processing dataset configuration: arithmetic_1dc\n",
      "Processing dataset configuration: arithmetic_2da\n",
      "{'question': 'Question: What is 31 plus 72?\\nAnswer:', 'final_answer': '<tool:calculator>31 + 72</tool>'}\n",
      "{'question': 'Question: What is 12 plus 63?\\nAnswer:', 'final_answer': '<tool:calculator>12 + 63</tool>'}\n",
      "{'question': 'Question: What is 46 plus 53?\\nAnswer:', 'final_answer': '<tool:calculator>46 + 53</tool>'}\n",
      "{'question': 'Question: What is 46 plus 53?\\nAnswer:', 'final_answer': '<tool:calculator>46 + 53</tool>'}\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "print_section(\"Loading Data\")\n",
    "# Prepare datasets\n",
    "dataset = prepare_arithmetic_datasets()\n",
    "train_data = dataset[\"train_dict\"]\n",
    "test_data = dataset[\"test_dict\"]\n",
    "train_transformed_data = dataset[\"train_transformed_dict\"]\n",
    "test_transformed_data = dataset[\"test_transformed_dict\"]\n",
    "print(train_transformed_data['arithmetic_2da'][3])\n",
    "print(test_transformed_data['arithmetic_2da'][3])\n",
    "print(train_transformed_data['arithmetic_2da'][-1])\n",
    "print(train_transformed_data['arithmetic_2da'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525ff7c9",
   "metadata": {},
   "source": [
    "# Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315c4980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRETAINED MODEL EVALUATION\n",
    "print_section(\"Pretrained Model Evaluation\")\n",
    "eval_model(MODEL_NAME, DATASET, test_data, model, tokenizer, use_tool=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081a6677",
   "metadata": {},
   "source": [
    "# Toolformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13379eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[032m==================================================\u001b[0m\n",
      "\u001b[032m\n",
      "Toolformer Fine Tuning Training\n",
      "\u001b[0m\n",
      "\u001b[032m==================================================\u001b[0m\n",
      "Combining arithmetic datasets for training...\n",
      "Combined 3600 examples for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3600/3600 [00:00<00:00, 8018.30 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3600/3600 [00:00<00:00, 516080.88 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3600 examples to data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2/2 [00:00<00:00, 477.96 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created evaluation dataset with 2 examples for monitoring\n",
      "Starting fresh training for 1 epochs\n",
      "Model Type: qwen2\n",
      "Tokenizer Type: Qwen2TokenizerFast\n",
      "When solving arithmetic problems, use the calculator tool by writing <tool:calculator>expression</tool>.\n",
      "Question: What is (5 + 1) + 1?\n",
      "Answer:<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "<tool:calculator>(5 + 1) + 1</tool><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "When solving arithmetic problems, use the calculator tool by writing <tool:calculator>expression</tool>.\n",
      "Question: What is (4 - 2) + 7?\n",
      "Answer:<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "<tool:calculator>(4 - 2) + 7</tool><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdaniel-chuang\u001b[0m (\u001b[33mdaniel-chuang-cornell\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\user\\Desktop\\Programming\\Classes\\CS4782\\final_project\\code\\wandb\\run-20250512_162859-blm7l7r4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/daniel-chuang-cornell/toolformer/runs/blm7l7r4' target=\"_blank\">toolformer_training_20250512_162859</a></strong> to <a href='https://wandb.ai/daniel-chuang-cornell/toolformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/daniel-chuang-cornell/toolformer' target=\"_blank\">https://wandb.ai/daniel-chuang-cornell/toolformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/daniel-chuang-cornell/toolformer/runs/blm7l7r4' target=\"_blank\">https://wandb.ai/daniel-chuang-cornell/toolformer/runs/blm7l7r4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/31 01:54, Epoch 0.99/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "DynamicCache + torch.export is tested on torch 2.6.0+ and may not work on earlier versions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking tool usage at step 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Question: What is 7 + 8?\n",
      "Answer:\n",
      "Output: Question: What is 7 + 8?\n",
      "Answer: 15\n",
      "Explain how we arrive at this answer: To find the answer, we need to follow these steps:\n",
      "\n",
      "1. Identify the numbers\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n",
      "Input: Question: What is 12 * 3?\n",
      "Answer:\n",
      "Output: Question: What is 12 * 3?\n",
      "Answer: 36\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking tool usage at step 4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Question: What is 7 + 8?\n",
      "Answer:\n",
      "Output: Question: What is 7 + 8?\n",
      "Answer: 15\n",
      "Explain how we arrive at this answer: 7 + 8 = 15, which is the sum of 7\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n",
      "Input: Question: What is 12 * 3?\n",
      "Answer:\n",
      "Output: Question: What is 12 * 3?\n",
      "Answer: 36\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking tool usage at step 6 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Question: What is 7 + 8?\n",
      "Answer:\n",
      "Output: Question: What is 7 + 8?\n",
      "Answer: 15\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n",
      "Input: Question: What is 12 * 3?\n",
      "Answer:\n",
      "Output: Question: What is 12 * 3?\n",
      "Answer: 36\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking tool usage at step 8 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Question: What is 7 + 8?\n",
      "Answer:\n",
      "Output: Question: What is 7 + 8?\n",
      "Answer: 15\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n",
      "Input: Question: What is 12 * 3?\n",
      "Answer:\n",
      "Output: Question: What is 12 * 3?\n",
      "Answer: 36\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking tool usage at step 10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Question: What is 7 + 8?\n",
      "Answer:\n",
      "Output: Question: What is 7 + 8?\n",
      "Answer: 15\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n",
      "Input: Question: What is 12 * 3?\n",
      "Answer:\n",
      "Output: Question: What is 12 * 3?\n",
      "Answer: 12 * 3 = 36\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking tool usage at step 12 ---\n",
      "Input: Question: What is 7 + 8?\n",
      "Answer:\n",
      "Output: Question: What is 7 + 8?\n",
      "Answer: 9\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n",
      "Input: Question: What is 12 * 3?\n",
      "Answer:\n",
      "Output: Question: What is 12 * 3?\n",
      "Answer: 12 * 3 = 36\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking tool usage at step 14 ---\n",
      "Input: Question: What is 7 + 8?\n",
      "Answer:\n",
      "Output: Question: What is 7 + 8?\n",
      "Answer: 9\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n",
      "Input: Question: What is 12 * 3?\n",
      "Answer:\n",
      "Output: Question: What is 12 * 3?\n",
      "Answer: 12 * 3 = 36\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking tool usage at step 16 ---\n",
      "Input: Question: What is 7 + 8?\n",
      "Answer:\n",
      "Output: Question: What is 7 + 8?\n",
      "Answer: 9\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n",
      "Input: Question: What is 12 * 3?\n",
      "Answer:\n",
      "Output: Question: What is 12 * 3?\n",
      "Answer: 12 * 3 = 36\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking tool usage at step 18 ---\n",
      "Input: Question: What is 7 + 8?\n",
      "Answer:\n",
      "Output: Question: What is 7 + 8?\n",
      "Answer: 9\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n",
      "Input: Question: What is 12 * 3?\n",
      "Answer:\n",
      "Output: Question: What is 12 * 3?\n",
      "Answer: 12 * 3 = 36\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking tool usage at step 20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Question: What is 7 + 8?\n",
      "Answer:\n",
      "Output: Question: What is 7 + 8?\n",
      "Answer: 7 + 8 = 15\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n",
      "Input: Question: What is 12 * 3?\n",
      "Answer:\n",
      "Output: Question: What is 12 * 3?\n",
      "Answer: 12 * 3 = 36.\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking tool usage at step 22 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Question: What is 7 + 8?\n",
      "Answer:\n",
      "Output: Question: What is 7 + 8?\n",
      "Answer: 7 + 8 = 15\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n",
      "Input: Question: What is 12 * 3?\n",
      "Answer:\n",
      "Output: Question: What is 12 * 3?\n",
      "Answer: 12 * 3 = 36.\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking tool usage at step 24 ---\n",
      "Input: Question: What is 7 + 8?\n",
      "Answer:\n",
      "Output: Question: What is 7 + 8?\n",
      "Answer: 9\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n",
      "Input: Question: What is 12 * 3?\n",
      "Answer:\n",
      "Output: Question: What is 12 * 3?\n",
      "Answer: 12 * 3 = 36.\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking tool usage at step 26 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Question: What is 7 + 8?\n",
      "Answer:\n",
      "Output: Question: What is 7 + 8?\n",
      "Answer: 7 + 8 = 15\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n",
      "Input: Question: What is 12 * 3?\n",
      "Answer:\n",
      "Output: Question: What is 12 * 3?\n",
      "Answer: 12 * 3 = 36\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking tool usage at step 28 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Question: What is 7 + 8?\n",
      "Answer:\n",
      "Output: Question: What is 7 + 8?\n",
      "Answer: 9\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n",
      "Input: Question: What is 12 * 3?\n",
      "Answer:\n",
      "Output: Question: What is 12 * 3?\n",
      "Answer: 12 * 3 = 36\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking tool usage at step 30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Question: What is 7 + 8?\n",
      "Answer:\n",
      "Output: Question: What is 7 + 8?\n",
      "Answer: 7 + 8 = 15\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n",
      "Input: Question: What is 12 * 3?\n",
      "Answer:\n",
      "Output: Question: What is 12 * 3?\n",
      "Answer: 12 * 3 = 36\n",
      "Uses tool: False\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TOOLFORMER FINE TUNING TRAINING\n",
    "print_section(\"Toolformer Fine Tuning Training\")\n",
    "\n",
    "# Prepare the training data based on dataset type\n",
    "train_dataset = combine_and_tokenize(train_transformed_data, tokenizer, path=TOOL_TRAIN_DATASET_PATH)\n",
    "train_dataset = train_dataset.shuffle(seed=42).select(range(data_points))\n",
    "\n",
    "# Create a small evaluation dataset directly instead of using combine_and_tokenize\n",
    "eval_examples = []\n",
    "for config_name, config_dataset in test_transformed_data.items():\n",
    "    # Take at most 5 examples from each configuration\n",
    "    sample_size = 1\n",
    "    for i in range(sample_size):\n",
    "        if isinstance(config_dataset[i], dict):\n",
    "            eval_examples.append({\n",
    "                \"question\": config_dataset[i][\"question\"],\n",
    "                \"final_answer\": config_dataset[i][\"final_answer\"]\n",
    "            })\n",
    "\n",
    "# Create the evaluation dataset directly\n",
    "eval_dataset = Dataset.from_list(eval_examples)\n",
    "eval_dataset = eval_dataset.map(\n",
    "    lambda examples: preprocess_for_training(examples, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\"Created evaluation dataset with {len(eval_dataset)} examples for monitoring\")\n",
    "\n",
    "# Load previous model if it exists\n",
    "try:\n",
    "    previous_path = os.path.join(CHECKPOINTS, \"finetuned\", TOOL_FINETUNED_SAVE_PATH)\n",
    "    tokenizer, model, metadata = load_model(previous_path)\n",
    "    \n",
    "    # Get total epochs from metadata\n",
    "    total_epochs = metadata.get(\"total_epochs\", 0) + current_epochs_tool\n",
    "    print(f\"Continuing training from {metadata.get('total_epochs', 0)} epochs to {total_epochs} epochs\")\n",
    "except FileNotFoundError:\n",
    "    # Start fresh training\n",
    "    total_epochs = current_epochs_tool\n",
    "    print(f\"Starting fresh training for {current_epochs_tool} epochs\")\n",
    "\n",
    "# Train the model\n",
    "model, tokenizer, metadata = train_model(model, tokenizer, train_dataset, num_epochs=current_epochs_tool, eval_dataset=eval_dataset)\n",
    "\n",
    "# Save with updated epoch count\n",
    "saved_path = save_model(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    os.path.join(CHECKPOINTS, \"finetuned\", TOOL_FINETUNED_SAVE_PATH),\n",
    "    epochs=current_epochs_tool,\n",
    "    total_epochs=total_epochs\n",
    ")\n",
    "print(f\"Saved fine-tuned model to {saved_path} (Total epochs: {total_epochs})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d134b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"Latest Checkpoint Evaluation\")\n",
    "model, metadata = load_model(os.path.join(os.curdir, \"toolformer_model\", \"checkpoint-225\"))\n",
    "print_section(\"Most recent training Model Evaluation\")\n",
    "eval_model(MODEL_NAME, DATASET, test_data, model, tokenizer, use_tool=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2b250a",
   "metadata": {},
   "source": [
    "# Pure Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5334a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PURE FINE TUNING TRAINING\n",
    "print_section(\"Pure Fine Tuning Training\")\n",
    "\n",
    "# Prepare the training data based on dataset type\n",
    "train_dataset = combine_and_tokenize(train_data, tokenizer, path=PURE_TRAIN_DATASET_PATH)\n",
    "\n",
    "# Load previous model if it exists\n",
    "try:\n",
    "    previous_path = os.path.join(CHECKPOINTS, \"finetuned\", TOOL_FINETUNED_SAVE_PATH)\n",
    "    tokenizer, model, metadata = load_model(previous_path)\n",
    "    \n",
    "    # Get total epochs from metadata\n",
    "    total_epochs = metadata.get(\"total_epochs\", 0) + current_epochs_pure\n",
    "    print(f\"Continuing training from {metadata.get('total_epochs', 0)} epochs to {total_epochs} epochs\")\n",
    "except FileNotFoundError:\n",
    "    # Start fresh training\n",
    "    total_epochs = current_epochs_pure\n",
    "    print(f\"Starting fresh training for {current_epochs_pure} epochs\")\n",
    "\n",
    "# Train the model\n",
    "model, tokenizer, metadata = train_model(model, tokenizer, train_dataset, num_epochs=current_epochs_pure)\n",
    "\n",
    "# Save with updated epoch count\n",
    "saved_path = save_model(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    os.path.join(CHECKPOINTS, \"finetuned\", TOOL_FINETUNED_SAVE_PATH),\n",
    "    epochs=current_epochs_pure,\n",
    "    total_epochs=total_epochs\n",
    ")\n",
    "print(f\"Saved fine-tuned model to {saved_path} (Total epochs: {total_epochs})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752e89c1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
