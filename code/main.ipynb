{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "116a82f1",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e772023f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from model.setup import setup_model, save_model, load_model, load_tokenizer\n",
    "from utils.console import isYes, printc, inputc, print_section\n",
    "from model.training import preprocess_for_training, train_model\n",
    "from model.custom_training import train_toolformer_model\n",
    "from data.gsm8k import prepare_gsm8k_dataset\n",
    "from data.svamp import prepare_svamp_dataset\n",
    "from data.arithmetic import prepare_arithmetic_datasets\n",
    "from evaluation.math_evaluation import evaluate_math_performance\n",
    "from evaluation.eval_pipeline import eval_model\n",
    "from constants import MODEL_NAME, INITIAL_SAVE_PATH, TOOL_FINETUNED_SAVE_PATH, DATASET, CHECKPOINTS, TOOL_TRAIN_DATASET_PATH, PURE_TRAIN_DATASET_PATH, EVAL_DATASET_PATH\n",
    "from data.arithmetic import combine_and_tokenize\n",
    "import wandb\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1ea1fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "current_epochs_tool = 1\n",
    "current_epochs_pure = 1\n",
    "data_points = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3516b64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[032m==================================================\u001b[0m\n",
      "\u001b[032m\n",
      "Loading Model\n",
      "\u001b[0m\n",
      "\u001b[032m==================================================\u001b[0m\n",
      "Loading model from local path: ./checkpoints\\pretrained\\qwen-initial\n",
      "Loaded model from saved path\n",
      "\u001b[032m==================================================\u001b[0m\n",
      "\u001b[032m\n",
      "Adding Tool Tokens\n",
      "\u001b[0m\n",
      "\u001b[032m==================================================\u001b[0m\n",
      "Added 2 special tokens to the tokenizer\n",
      "Resized model embeddings to 151648 tokens\n",
      "Special tokens: ['<|endoftext|>', '<tool:calculator>', '</tool>']\n"
     ]
    }
   ],
   "source": [
    "print_section(\"Loading Model\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# Try to load from saved path first, if it fails, download from HF\n",
    "try:\n",
    "    model, metadata = load_model(os.path.join(CHECKPOINTS, \"pretrained\", INITIAL_SAVE_PATH))\n",
    "    tokenizer = load_tokenizer(os.path.join(CHECKPOINTS, \"pretrained\", INITIAL_SAVE_PATH))\n",
    "    print(\"Loaded model from saved path\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Initial model not found. Setting up from {MODEL_NAME}\")\n",
    "    tokenizer, model, metadata = setup_model(MODEL_NAME)\n",
    "    save_model(model, tokenizer, os.path.join(CHECKPOINTS, \"pretrained\", INITIAL_SAVE_PATH))\n",
    "\n",
    "print_section(\"Adding Tool Tokens\")\n",
    "tool_tokens = {\n",
    "    \"additional_special_tokens\": [\n",
    "        \"<tool:calculator>\",\n",
    "        \"</tool>\",\n",
    "    ]\n",
    "}\n",
    "num_added = tokenizer.add_special_tokens(tool_tokens)\n",
    "print(f\"Added {num_added} special tokens to the tokenizer\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"Resized model embeddings to {len(tokenizer)} tokens\")\n",
    "print(\"Special tokens:\", tokenizer.all_special_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a320640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[032m==================================================\u001b[0m\n",
      "\u001b[032m\n",
      "Loading Data\n",
      "\u001b[0m\n",
      "\u001b[032m==================================================\u001b[0m\n",
      "Processing dataset configuration: arithmetic_1dc\n",
      "Processing dataset configuration: arithmetic_2da\n",
      "{'question': 'Question: What is 31 plus 72?\\nAnswer:', 'final_answer': '<tool:calculator>31 + 72</tool>'}\n",
      "{'question': 'Question: What is 12 plus 63?\\nAnswer:', 'final_answer': '<tool:calculator>12 + 63</tool>'}\n",
      "{'question': 'Question: What is 46 plus 53?\\nAnswer:', 'final_answer': '<tool:calculator>46 + 53</tool>'}\n",
      "{'question': 'Question: What is 46 plus 53?\\nAnswer:', 'final_answer': '<tool:calculator>46 + 53</tool>'}\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "print_section(\"Loading Data\")\n",
    "# Prepare datasets\n",
    "dataset = prepare_arithmetic_datasets()\n",
    "train_data = dataset[\"train_dict\"]\n",
    "test_data = dataset[\"test_dict\"]\n",
    "train_transformed_data = dataset[\"train_transformed_dict\"]\n",
    "test_transformed_data = dataset[\"test_transformed_dict\"]\n",
    "print(train_transformed_data['arithmetic_2da'][3])\n",
    "print(test_transformed_data['arithmetic_2da'][3])\n",
    "print(train_transformed_data['arithmetic_2da'][-1])\n",
    "print(train_transformed_data['arithmetic_2da'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525ff7c9",
   "metadata": {},
   "source": [
    "# Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315c4980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRETAINED MODEL EVALUATION\n",
    "print_section(\"Pretrained Model Evaluation\")\n",
    "eval_model(MODEL_NAME, DATASET, test_data, model, tokenizer, use_tool=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081a6677",
   "metadata": {},
   "source": [
    "# Toolformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13379eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[032m==================================================\u001b[0m\n",
      "\u001b[032m\n",
      "Toolformer Fine Tuning Training\n",
      "\u001b[0m\n",
      "\u001b[032m==================================================\u001b[0m\n",
      "Loaded 3600 examples from .preprocessed/preprocessed_train_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 3856.83 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created evaluation dataset with 50 examples\n",
      "Training dataset size: 1000\n",
      "Evaluation dataset size: 50\n",
      "\n",
      "=== Sample Training Data ===\n",
      "{'input_ids': [4498, 21828, 34784, 5322, 11, 990, 279, 29952, 5392, 553, 4378, 220, 151646, 28099, 151647, 624, 14582, 25, 3555, 374, 320, 20, 488, 220, 16, 8, 488, 220, 16, 5267, 16141, 25, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [151646, 7, 20, 488, 220, 16, 8, 488, 220, 16, 151647, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643]}\n",
      "Starting fresh training for 1 epochs\n",
      "Model Type: qwen2\n",
      "Tokenizer Type: Qwen2TokenizerFast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting 3 examples for SFTTrainer\n",
      "<class 'datasets.formatting.formatting.LazyBatch'>\n",
      "Available keys: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'question'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting fresh training for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_epochs_tool\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m epochs\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Train the model using the updated train_model function (now using SFTTrainer)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m model, tokenizer, metadata = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_epochs_tool\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Save with updated epoch count\u001b[39;00m\n\u001b[32m     42\u001b[39m saved_path = save_model(\n\u001b[32m     43\u001b[39m     model, \n\u001b[32m     44\u001b[39m     tokenizer, \n\u001b[32m   (...)\u001b[39m\u001b[32m     47\u001b[39m     total_epochs=total_epochs\n\u001b[32m     48\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Programming\\Classes\\CS4782\\final_project\\code\\model\\training.py:82\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, tokenizer, train_dataset, eval_dataset, output_dir, num_epochs, previous_metadata)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTokenizer Type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tokenizer).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Convert datasets to SFT format\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m sft_train_dataset = \u001b[43mconvert_to_sft_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m sft_eval_dataset = convert_to_sft_format(eval_dataset, tokenizer) \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Print samples for debugging\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Programming\\Classes\\CS4782\\final_project\\code\\model\\training.py:43\u001b[39m, in \u001b[36mconvert_to_sft_format\u001b[39m\u001b[34m(dataset, tokenizer)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Convert dataset to the format expected by SFTTrainer\"\"\"\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Apply the formatting function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m formatted_dataset = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_messages_for_sft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Apply chat template\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_template\u001b[39m(example):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m self_format = {\n\u001b[32m    551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    555\u001b[39m }\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\datasets\\arrow_dataset.py:3079\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3074\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[32m   3075\u001b[39m         unit=\u001b[33m\"\u001b[39m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3076\u001b[39m         total=pbar_total,\n\u001b[32m   3077\u001b[39m         desc=desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMap\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3078\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m3079\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\datasets\\arrow_dataset.py:3525\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3523\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3524\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3525\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3526\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_examples_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3527\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\datasets\\arrow_dataset.py:3475\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3473\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3474\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3475\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\datasets\\arrow_dataset.py:3398\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3396\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3397\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3398\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3399\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\Programming\\Classes\\CS4782\\final_project\\code\\model\\training.py:30\u001b[39m, in \u001b[36mformat_messages_for_sft\u001b[39m\u001b[34m(examples)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(examples))\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAvailable keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(examples.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m question, answer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, examples[\u001b[33m\"\u001b[39m\u001b[33mfinal_answer\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m     31\u001b[39m     messages = [\n\u001b[32m     32\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: system_prompt},\n\u001b[32m     33\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: question},\n\u001b[32m     34\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: answer}\n\u001b[32m     35\u001b[39m     ]\n\u001b[32m     36\u001b[39m     formatted_texts.append(messages)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\scoop\\apps\\anaconda3\\current\\envs\\torch-gpu3\\Lib\\site-packages\\datasets\\formatting\\formatting.py:278\u001b[39m, in \u001b[36mLazyDict.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.keys_to_format:\n\u001b[32m    280\u001b[39m         value = \u001b[38;5;28mself\u001b[39m.format(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'question'"
     ]
    }
   ],
   "source": [
    "# TOOLFORMER FINE TUNING TRAINING\n",
    "print_section(\"Toolformer Fine Tuning Training\")\n",
    "\n",
    "# Prepare the training data based on dataset type\n",
    "train_dataset = combine_and_tokenize(train_transformed_data, tokenizer, path=TOOL_TRAIN_DATASET_PATH)\n",
    "\n",
    "# Create a small evaluation dataset directly instead of using combine_and_tokenize\n",
    "eval_examples = []\n",
    "for config_name, config_dataset in test_transformed_data.items():\n",
    "    # Take at most 5 examples from each configuration\n",
    "    sample_size = 1\n",
    "    for i in range(sample_size):\n",
    "        if isinstance(config_dataset[i], dict):\n",
    "            eval_examples.append({\n",
    "                \"question\": config_dataset[i][\"question\"],\n",
    "                \"final_answer\": config_dataset[i][\"final_answer\"]\n",
    "            })\n",
    "\n",
    "# Create the evaluation dataset directly\n",
    "eval_dataset = Dataset.from_list(eval_examples)\n",
    "eval_dataset = eval_dataset.map(\n",
    "    lambda examples: preprocess_for_training(examples, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\"Created evaluation dataset with {len(eval_dataset)} examples for monitoring\")\n",
    "\n",
    "# Load previous model if it exists\n",
    "try:\n",
    "    previous_path = os.path.join(CHECKPOINTS, \"finetuned\", TOOL_FINETUNED_SAVE_PATH)\n",
    "    tokenizer, model, metadata = load_model(previous_path)\n",
    "    \n",
    "    # Get total epochs from metadata\n",
    "    total_epochs = metadata.get(\"total_epochs\", 0) + current_epochs_tool\n",
    "    print(f\"Continuing training from {metadata.get('total_epochs', 0)} epochs to {total_epochs} epochs\")\n",
    "except FileNotFoundError:\n",
    "    # Start fresh training\n",
    "    total_epochs = current_epochs_tool\n",
    "    print(f\"Starting fresh training for {current_epochs_tool} epochs\")\n",
    "\n",
    "# Train the model\n",
    "model, tokenizer, metadata = train_model(model, tokenizer, train_dataset, num_epochs=current_epochs_tool, eval_dataset=eval_dataset)\n",
    "\n",
    "# Save with updated epoch count\n",
    "saved_path = save_model(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    os.path.join(CHECKPOINTS, \"finetuned\", TOOL_FINETUNED_SAVE_PATH),\n",
    "    epochs=current_epochs_tool,\n",
    "    total_epochs=total_epochs\n",
    ")\n",
    "print(f\"Saved fine-tuned model to {saved_path} (Total epochs: {total_epochs})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d134b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"Latest Checkpoint Evaluation\")\n",
    "model, metadata = load_model(os.path.join(os.curdir, \"toolformer_model\", \"checkpoint-225\"))\n",
    "print_section(\"Most recent training Model Evaluation\")\n",
    "eval_model(MODEL_NAME, DATASET, test_data, model, tokenizer, use_tool=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2b250a",
   "metadata": {},
   "source": [
    "# Pure Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5334a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PURE FINE TUNING TRAINING\n",
    "print_section(\"Pure Fine Tuning Training\")\n",
    "\n",
    "# Prepare the training data based on dataset type\n",
    "train_dataset = combine_and_tokenize(train_data, tokenizer, path=PURE_TRAIN_DATASET_PATH)\n",
    "\n",
    "# Load previous model if it exists\n",
    "try:\n",
    "    previous_path = os.path.join(CHECKPOINTS, \"finetuned\", TOOL_FINETUNED_SAVE_PATH)\n",
    "    tokenizer, model, metadata = load_model(previous_path)\n",
    "    \n",
    "    # Get total epochs from metadata\n",
    "    total_epochs = metadata.get(\"total_epochs\", 0) + current_epochs_pure\n",
    "    print(f\"Continuing training from {metadata.get('total_epochs', 0)} epochs to {total_epochs} epochs\")\n",
    "except FileNotFoundError:\n",
    "    # Start fresh training\n",
    "    total_epochs = current_epochs_pure\n",
    "    print(f\"Starting fresh training for {current_epochs_pure} epochs\")\n",
    "\n",
    "# Train the model\n",
    "model, tokenizer, metadata = train_model(model, tokenizer, train_dataset, num_epochs=current_epochs_pure)\n",
    "\n",
    "# Save with updated epoch count\n",
    "saved_path = save_model(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    os.path.join(CHECKPOINTS, \"finetuned\", TOOL_FINETUNED_SAVE_PATH),\n",
    "    epochs=current_epochs_pure,\n",
    "    total_epochs=total_epochs\n",
    ")\n",
    "print(f\"Saved fine-tuned model to {saved_path} (Total epochs: {total_epochs})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752e89c1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
